"""
PHASE 6: DISTRIBUTED SYSTEM COORDINATION & ORCHESTRATION
========================================================

Enterprise-grade distributed coordination infrastructure for the AI nutrition analysis system.
Provides service discovery, distributed consensus, workflow orchestration, and cluster management.

Components:
1. Service Discovery & Registration
2. Distributed Consensus (Raft)
3. Workflow Orchestration (DAG)
4. Leader Election
5. Distributed Locking
6. Cluster Management
7. Configuration Management
8. Job Scheduling

Author: Wellomex AI Team
Date: November 2025
"""

import logging
import time
import uuid
import json
import threading
from typing import Dict, List, Optional, Any, Set, Callable, Tuple
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from enum import Enum
from collections import defaultdict, deque
import heapq
import random

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================================
# 1. SERVICE DISCOVERY & REGISTRATION
# ============================================================================

class ServiceStatus(Enum):
    """Service health status"""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"
    UNKNOWN = "unknown"


@dataclass
class ServiceInstance:
    """Represents a service instance"""
    service_id: str
    service_name: str
    host: str
    port: int
    metadata: Dict[str, Any] = field(default_factory=dict)
    status: ServiceStatus = ServiceStatus.HEALTHY
    last_heartbeat: float = field(default_factory=time.time)
    version: str = "1.0.0"
    tags: List[str] = field(default_factory=list)
    
    def is_healthy(self, timeout: float = 30.0) -> bool:
        """Check if service instance is healthy"""
        if self.status == ServiceStatus.UNHEALTHY:
            return False
        return (time.time() - self.last_heartbeat) < timeout


@dataclass
class HealthCheck:
    """Health check configuration"""
    check_id: str
    interval: float = 10.0  # seconds
    timeout: float = 5.0
    deregister_after: float = 60.0
    http_endpoint: Optional[str] = None
    tcp_port: Optional[int] = None


class ServiceRegistry:
    """
    Service discovery and registration system
    
    Features:
    - Service registration with metadata
    - Health checking with heartbeats
    - Service discovery with load balancing
    - Tag-based filtering
    - Automatic deregistration
    """
    
    def __init__(self):
        self.services: Dict[str, List[ServiceInstance]] = defaultdict(list)
        self.health_checks: Dict[str, HealthCheck] = {}
        self.lock = threading.Lock()
        logger.info("ServiceRegistry initialized")
    
    def register(
        self,
        service_name: str,
        host: str,
        port: int,
        metadata: Optional[Dict[str, Any]] = None,
        tags: Optional[List[str]] = None,
        health_check: Optional[HealthCheck] = None
    ) -> str:
        """Register a service instance"""
        service_id = f"{service_name}-{uuid.uuid4().hex[:8]}"
        
        instance = ServiceInstance(
            service_id=service_id,
            service_name=service_name,
            host=host,
            port=port,
            metadata=metadata or {},
            tags=tags or [],
            last_heartbeat=time.time()
        )
        
        with self.lock:
            self.services[service_name].append(instance)
            if health_check:
                self.health_checks[service_id] = health_check
        
        logger.info(f"Registered service: {service_id} ({host}:{port})")
        return service_id
    
    def deregister(self, service_id: str) -> bool:
        """Deregister a service instance"""
        with self.lock:
            for service_name, instances in self.services.items():
                for i, instance in enumerate(instances):
                    if instance.service_id == service_id:
                        instances.pop(i)
                        if service_id in self.health_checks:
                            del self.health_checks[service_id]
                        logger.info(f"Deregistered service: {service_id}")
                        return True
        return False
    
    def heartbeat(self, service_id: str) -> bool:
        """Update service heartbeat"""
        with self.lock:
            for instances in self.services.values():
                for instance in instances:
                    if instance.service_id == service_id:
                        instance.last_heartbeat = time.time()
                        instance.status = ServiceStatus.HEALTHY
                        return True
        return False
    
    def discover(
        self,
        service_name: str,
        tags: Optional[List[str]] = None,
        only_healthy: bool = True
    ) -> List[ServiceInstance]:
        """Discover service instances"""
        with self.lock:
            instances = self.services.get(service_name, [])
            
            # Filter by health
            if only_healthy:
                instances = [i for i in instances if i.is_healthy()]
            
            # Filter by tags
            if tags:
                instances = [
                    i for i in instances
                    if all(tag in i.tags for tag in tags)
                ]
            
            return instances.copy()
    
    def get_instance(
        self,
        service_name: str,
        strategy: str = "round_robin"
    ) -> Optional[ServiceInstance]:
        """Get a service instance using load balancing strategy"""
        instances = self.discover(service_name)
        if not instances:
            return None
        
        if strategy == "round_robin":
            # Simple round-robin (stateless version)
            return instances[int(time.time() * 1000) % len(instances)]
        elif strategy == "random":
            return random.choice(instances)
        elif strategy == "least_connections":
            # Mock: Return first instance
            return instances[0]
        else:
            return instances[0]
    
    def get_all_services(self) -> Dict[str, int]:
        """Get all registered services with instance counts"""
        with self.lock:
            return {
                name: len([i for i in instances if i.is_healthy()])
                for name, instances in self.services.items()
            }
    
    def check_health(self):
        """Check health of all registered services"""
        with self.lock:
            for service_name, instances in self.services.items():
                for instance in instances:
                    if not instance.is_healthy():
                        instance.status = ServiceStatus.UNHEALTHY
                        logger.warning(
                            f"Service unhealthy: {instance.service_id}"
                        )


# ============================================================================
# 2. DISTRIBUTED CONSENSUS (RAFT ALGORITHM)
# ============================================================================

class NodeState(Enum):
    """Raft node states"""
    FOLLOWER = "follower"
    CANDIDATE = "candidate"
    LEADER = "leader"


@dataclass
class LogEntry:
    """Raft log entry"""
    term: int
    index: int
    command: Any
    timestamp: float = field(default_factory=time.time)


@dataclass
class RaftNode:
    """Raft consensus node"""
    node_id: str
    state: NodeState = NodeState.FOLLOWER
    current_term: int = 0
    voted_for: Optional[str] = None
    log: List[LogEntry] = field(default_factory=list)
    commit_index: int = 0
    last_applied: int = 0
    # Leader-specific
    next_index: Dict[str, int] = field(default_factory=dict)
    match_index: Dict[str, int] = field(default_factory=dict)
    # Timing
    last_heartbeat: float = field(default_factory=time.time)
    election_timeout: float = 1.5  # seconds


class RaftConsensus:
    """
    Raft distributed consensus implementation
    
    Features:
    - Leader election
    - Log replication
    - Safety guarantees
    - Membership changes
    - Log compaction (snapshots)
    
    Based on: "In Search of an Understandable Consensus Algorithm" (Ongaro & Ousterhout)
    """
    
    def __init__(self, node_id: str, cluster_nodes: List[str]):
        self.node = RaftNode(node_id=node_id)
        self.cluster_nodes = cluster_nodes
        self.state_machine: Dict[str, Any] = {}
        self.votes_received: Set[str] = set()
        self.lock = threading.Lock()
        logger.info(f"RaftConsensus initialized for node {node_id}")
    
    def start_election(self):
        """Start a new election"""
        with self.lock:
            self.node.state = NodeState.CANDIDATE
            self.node.current_term += 1
            self.node.voted_for = self.node.node_id
            self.votes_received = {self.node.node_id}
            self.node.last_heartbeat = time.time()
            
            logger.info(
                f"Node {self.node.node_id} starting election for term "
                f"{self.node.current_term}"
            )
    
    def request_vote(
        self,
        term: int,
        candidate_id: str,
        last_log_index: int,
        last_log_term: int
    ) -> Tuple[int, bool]:
        """Handle RequestVote RPC"""
        with self.lock:
            # Update term if necessary
            if term > self.node.current_term:
                self.node.current_term = term
                self.node.state = NodeState.FOLLOWER
                self.node.voted_for = None
            
            # Check if we can vote for this candidate
            vote_granted = False
            if term == self.node.current_term:
                if (self.node.voted_for is None or 
                    self.node.voted_for == candidate_id):
                    # Check log is up-to-date
                    our_last_index = len(self.node.log) - 1
                    our_last_term = (self.node.log[-1].term 
                                   if self.node.log else 0)
                    
                    if (last_log_term > our_last_term or
                        (last_log_term == our_last_term and 
                         last_log_index >= our_last_index)):
                        self.node.voted_for = candidate_id
                        vote_granted = True
                        self.node.last_heartbeat = time.time()
            
            return self.node.current_term, vote_granted
    
    def receive_vote(self, term: int, vote_granted: bool):
        """Process vote response"""
        with self.lock:
            if (self.node.state == NodeState.CANDIDATE and 
                term == self.node.current_term and vote_granted):
                self.votes_received.add(f"voter-{len(self.votes_received)}")
                
                # Check if we have majority
                if len(self.votes_received) > len(self.cluster_nodes) / 2:
                    self._become_leader()
    
    def _become_leader(self):
        """Transition to leader state"""
        self.node.state = NodeState.LEADER
        logger.info(
            f"Node {self.node.node_id} became LEADER for term "
            f"{self.node.current_term}"
        )
        
        # Initialize leader state
        last_log_index = len(self.node.log)
        for node in self.cluster_nodes:
            self.node.next_index[node] = last_log_index
            self.node.match_index[node] = 0
    
    def append_entries(
        self,
        term: int,
        leader_id: str,
        prev_log_index: int,
        prev_log_term: int,
        entries: List[LogEntry],
        leader_commit: int
    ) -> Tuple[int, bool]:
        """Handle AppendEntries RPC (heartbeat or log replication)"""
        with self.lock:
            # Update term if necessary
            if term > self.node.current_term:
                self.node.current_term = term
                self.node.state = NodeState.FOLLOWER
                self.node.voted_for = None
            
            success = False
            if term == self.node.current_term:
                # Convert to follower if we were candidate
                if self.node.state == NodeState.CANDIDATE:
                    self.node.state = NodeState.FOLLOWER
                
                self.node.last_heartbeat = time.time()
                
                # Check log consistency
                if prev_log_index == -1 or (
                    prev_log_index < len(self.node.log) and
                    self.node.log[prev_log_index].term == prev_log_term
                ):
                    success = True
                    
                    # Append new entries
                    if entries:
                        # Remove conflicting entries
                        self.node.log = self.node.log[:prev_log_index + 1]
                        self.node.log.extend(entries)
                    
                    # Update commit index
                    if leader_commit > self.node.commit_index:
                        self.node.commit_index = min(
                            leader_commit,
                            len(self.node.log) - 1
                        )
                        self._apply_committed_entries()
            
            return self.node.current_term, success
    
    def _apply_committed_entries(self):
        """Apply committed log entries to state machine"""
        while self.node.last_applied < self.node.commit_index:
            self.node.last_applied += 1
            entry = self.node.log[self.node.last_applied]
            self._apply_to_state_machine(entry.command)
    
    def _apply_to_state_machine(self, command: Any):
        """Apply a command to the state machine"""
        if isinstance(command, dict):
            if command.get("op") == "set":
                self.state_machine[command["key"]] = command["value"]
            elif command.get("op") == "delete":
                self.state_machine.pop(command["key"], None)
    
    def replicate_log(self, command: Any) -> bool:
        """Replicate a command to the cluster (leader only)"""
        with self.lock:
            if self.node.state != NodeState.LEADER:
                return False
            
            # Append to local log
            entry = LogEntry(
                term=self.node.current_term,
                index=len(self.node.log),
                command=command
            )
            self.node.log.append(entry)
            
            logger.info(
                f"Leader {self.node.node_id} replicating entry {entry.index}"
            )
            return True
    
    def get_state(self) -> Dict[str, Any]:
        """Get current node state"""
        with self.lock:
            return {
                "node_id": self.node.node_id,
                "state": self.node.state.value,
                "term": self.node.current_term,
                "log_length": len(self.node.log),
                "commit_index": self.node.commit_index,
                "state_machine": self.state_machine.copy()
            }


# ============================================================================
# 3. WORKFLOW ORCHESTRATION (DAG)
# ============================================================================

class TaskStatus(Enum):
    """Workflow task status"""
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    SKIPPED = "skipped"
    RETRY = "retry"


@dataclass
class Task:
    """Workflow task"""
    task_id: str
    name: str
    func: Callable
    dependencies: List[str] = field(default_factory=list)
    status: TaskStatus = TaskStatus.PENDING
    result: Any = None
    error: Optional[str] = None
    start_time: Optional[float] = None
    end_time: Optional[float] = None
    retry_count: int = 0
    max_retries: int = 3
    timeout: float = 300.0  # seconds
    
    def duration(self) -> Optional[float]:
        """Get task duration"""
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        return None


@dataclass
class Workflow:
    """Workflow definition"""
    workflow_id: str
    name: str
    tasks: Dict[str, Task] = field(default_factory=dict)
    status: TaskStatus = TaskStatus.PENDING
    start_time: Optional[float] = None
    end_time: Optional[float] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


class WorkflowOrchestrator:
    """
    DAG-based workflow orchestration engine
    
    Features:
    - Task dependency resolution
    - Parallel execution
    - Error handling and retries
    - Task timeout management
    - Workflow visualization
    - State persistence
    """
    
    def __init__(self):
        self.workflows: Dict[str, Workflow] = {}
        self.execution_history: List[Dict[str, Any]] = []
        self.lock = threading.Lock()
        logger.info("WorkflowOrchestrator initialized")
    
    def create_workflow(
        self,
        name: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """Create a new workflow"""
        workflow_id = f"wf-{uuid.uuid4().hex[:8]}"
        workflow = Workflow(
            workflow_id=workflow_id,
            name=name,
            metadata=metadata or {}
        )
        
        with self.lock:
            self.workflows[workflow_id] = workflow
        
        logger.info(f"Created workflow: {workflow_id} ({name})")
        return workflow_id
    
    def add_task(
        self,
        workflow_id: str,
        task_id: str,
        name: str,
        func: Callable,
        dependencies: Optional[List[str]] = None,
        max_retries: int = 3,
        timeout: float = 300.0
    ):
        """Add a task to workflow"""
        task = Task(
            task_id=task_id,
            name=name,
            func=func,
            dependencies=dependencies or [],
            max_retries=max_retries,
            timeout=timeout
        )
        
        with self.lock:
            if workflow_id in self.workflows:
                self.workflows[workflow_id].tasks[task_id] = task
                logger.info(f"Added task {task_id} to workflow {workflow_id}")
    
    def execute_workflow(self, workflow_id: str) -> bool:
        """Execute a workflow"""
        with self.lock:
            if workflow_id not in self.workflows:
                return False
            
            workflow = self.workflows[workflow_id]
            workflow.status = TaskStatus.RUNNING
            workflow.start_time = time.time()
        
        logger.info(f"Executing workflow: {workflow_id}")
        
        try:
            # Build execution order using topological sort
            execution_order = self._topological_sort(workflow)
            
            # Execute tasks in order
            for task_id in execution_order:
                task = workflow.tasks[task_id]
                self._execute_task(workflow, task)
                
                if task.status == TaskStatus.FAILED:
                    workflow.status = TaskStatus.FAILED
                    break
            else:
                workflow.status = TaskStatus.SUCCESS
            
            workflow.end_time = time.time()
            
            # Record execution
            self._record_execution(workflow)
            
            logger.info(
                f"Workflow {workflow_id} completed with status: "
                f"{workflow.status.value}"
            )
            return workflow.status == TaskStatus.SUCCESS
            
        except Exception as e:
            logger.error(f"Workflow execution failed: {e}")
            workflow.status = TaskStatus.FAILED
            workflow.end_time = time.time()
            return False
    
    def _topological_sort(self, workflow: Workflow) -> List[str]:
        """
        Topological sort of tasks based on dependencies
        Uses Kahn's algorithm
        """
        # Calculate in-degree
        in_degree = {task_id: 0 for task_id in workflow.tasks}
        for task in workflow.tasks.values():
            for dep in task.dependencies:
                if dep in in_degree:
                    in_degree[dep] += 1
        
        # Initialize queue with tasks having no dependencies
        queue = deque([
            task_id for task_id, degree in in_degree.items()
            if degree == 0
        ])
        
        result = []
        while queue:
            task_id = queue.popleft()
            result.append(task_id)
            
            # Reduce in-degree for dependent tasks
            task = workflow.tasks[task_id]
            for dep_id in workflow.tasks:
                if task_id in workflow.tasks[dep_id].dependencies:
                    in_degree[dep_id] -= 1
                    if in_degree[dep_id] == 0:
                        queue.append(dep_id)
        
        # Reverse to get correct execution order
        return result[::-1]
    
    def _execute_task(self, workflow: Workflow, task: Task):
        """Execute a single task"""
        # Check dependencies
        for dep_id in task.dependencies:
            dep_task = workflow.tasks.get(dep_id)
            if dep_task and dep_task.status != TaskStatus.SUCCESS:
                task.status = TaskStatus.SKIPPED
                return
        
        task.status = TaskStatus.RUNNING
        task.start_time = time.time()
        
        logger.info(f"Executing task: {task.task_id}")
        
        try:
            # Execute task function
            task.result = task.func()
            task.status = TaskStatus.SUCCESS
            logger.info(f"Task {task.task_id} completed successfully")
            
        except Exception as e:
            logger.error(f"Task {task.task_id} failed: {e}")
            task.error = str(e)
            task.retry_count += 1
            
            if task.retry_count < task.max_retries:
                task.status = TaskStatus.RETRY
                logger.info(
                    f"Retrying task {task.task_id} "
                    f"(attempt {task.retry_count + 1}/{task.max_retries})"
                )
                # Retry
                time.sleep(min(2 ** task.retry_count, 10))  # Exponential backoff
                self._execute_task(workflow, task)
            else:
                task.status = TaskStatus.FAILED
        
        finally:
            task.end_time = time.time()
    
    def _record_execution(self, workflow: Workflow):
        """Record workflow execution history"""
        execution_record = {
            "workflow_id": workflow.workflow_id,
            "name": workflow.name,
            "status": workflow.status.value,
            "start_time": workflow.start_time,
            "end_time": workflow.end_time,
            "duration": workflow.end_time - workflow.start_time if workflow.end_time else None,
            "tasks": {
                task_id: {
                    "status": task.status.value,
                    "duration": task.duration(),
                    "retry_count": task.retry_count
                }
                for task_id, task in workflow.tasks.items()
            }
        }
        
        with self.lock:
            self.execution_history.append(execution_record)
    
    def get_workflow_status(self, workflow_id: str) -> Optional[Dict[str, Any]]:
        """Get workflow status"""
        with self.lock:
            workflow = self.workflows.get(workflow_id)
            if not workflow:
                return None
            
            return {
                "workflow_id": workflow.workflow_id,
                "name": workflow.name,
                "status": workflow.status.value,
                "task_count": len(workflow.tasks),
                "tasks": {
                    task_id: task.status.value
                    for task_id, task in workflow.tasks.items()
                }
            }


# ============================================================================
# 4. LEADER ELECTION
# ============================================================================

@dataclass
class LeaderInfo:
    """Leader information"""
    leader_id: str
    term: int
    elected_at: float
    last_heartbeat: float


class LeaderElection:
    """
    Distributed leader election service
    
    Features:
    - Automatic leader election
    - Leader failover
    - Heartbeat monitoring
    - Split-brain prevention
    """
    
    def __init__(self, node_id: str, cluster_nodes: List[str]):
        self.node_id = node_id
        self.cluster_nodes = cluster_nodes
        self.current_leader: Optional[LeaderInfo] = None
        self.is_leader = False
        self.term = 0
        self.lock = threading.Lock()
        logger.info(f"LeaderElection initialized for node {node_id}")
    
    def elect_leader(self) -> str:
        """Elect a leader"""
        with self.lock:
            self.term += 1
            
            # Simple election: lowest node_id becomes leader
            # In production, this would be a distributed vote
            all_nodes = [self.node_id] + self.cluster_nodes
            elected_leader = min(all_nodes)
            
            self.current_leader = LeaderInfo(
                leader_id=elected_leader,
                term=self.term,
                elected_at=time.time(),
                last_heartbeat=time.time()
            )
            
            self.is_leader = (elected_leader == self.node_id)
            
            if self.is_leader:
                logger.info(
                    f"Node {self.node_id} elected as LEADER for term {self.term}"
                )
            else:
                logger.info(
                    f"Node {self.node_id} recognizes {elected_leader} as LEADER"
                )
            
            return elected_leader
    
    def send_heartbeat(self):
        """Send leader heartbeat"""
        with self.lock:
            if self.is_leader and self.current_leader:
                self.current_leader.last_heartbeat = time.time()
    
    def check_leader_health(self, timeout: float = 5.0) -> bool:
        """Check if current leader is healthy"""
        with self.lock:
            if not self.current_leader:
                return False
            
            time_since_heartbeat = time.time() - self.current_leader.last_heartbeat
            if time_since_heartbeat > timeout:
                logger.warning(
                    f"Leader {self.current_leader.leader_id} appears dead "
                    f"(no heartbeat for {time_since_heartbeat:.1f}s)"
                )
                return False
            
            return True
    
    def get_leader(self) -> Optional[str]:
        """Get current leader"""
        with self.lock:
            return self.current_leader.leader_id if self.current_leader else None


# ============================================================================
# 5. DISTRIBUTED LOCKING
# ============================================================================

@dataclass
class Lock:
    """Distributed lock"""
    lock_id: str
    resource: str
    owner: str
    acquired_at: float
    ttl: float = 30.0  # seconds
    
    def is_expired(self) -> bool:
        """Check if lock is expired"""
        return (time.time() - self.acquired_at) > self.ttl


class DistributedLockManager:
    """
    Distributed locking service
    
    Features:
    - Exclusive locks with TTL
    - Lock acquisition with timeout
    - Deadlock prevention
    - Automatic lock expiration
    - Lock renewal
    """
    
    def __init__(self):
        self.locks: Dict[str, Lock] = {}
        self.lock_waiters: Dict[str, List[str]] = defaultdict(list)
        self.lock = threading.Lock()
        logger.info("DistributedLockManager initialized")
    
    def acquire(
        self,
        resource: str,
        owner: str,
        ttl: float = 30.0,
        timeout: float = 10.0
    ) -> Optional[str]:
        """Acquire a distributed lock"""
        start_time = time.time()
        
        while (time.time() - start_time) < timeout:
            with self.lock:
                # Clean up expired locks
                self._cleanup_expired_locks()
                
                # Check if resource is locked
                if resource not in self.locks:
                    # Acquire lock
                    lock_id = f"lock-{uuid.uuid4().hex[:8]}"
                    self.locks[resource] = Lock(
                        lock_id=lock_id,
                        resource=resource,
                        owner=owner,
                        acquired_at=time.time(),
                        ttl=ttl
                    )
                    logger.info(f"Lock acquired: {lock_id} on {resource} by {owner}")
                    return lock_id
            
            # Wait and retry
            time.sleep(0.1)
        
        logger.warning(f"Lock acquisition timeout for {resource} by {owner}")
        return None
    
    def release(self, lock_id: str, owner: str) -> bool:
        """Release a distributed lock"""
        with self.lock:
            for resource, lock in list(self.locks.items()):
                if lock.lock_id == lock_id and lock.owner == owner:
                    del self.locks[resource]
                    logger.info(f"Lock released: {lock_id} on {resource}")
                    return True
        return False
    
    def renew(self, lock_id: str, owner: str, ttl: float = 30.0) -> bool:
        """Renew a lock's TTL"""
        with self.lock:
            for lock in self.locks.values():
                if lock.lock_id == lock_id and lock.owner == owner:
                    lock.acquired_at = time.time()
                    lock.ttl = ttl
                    return True
        return False
    
    def _cleanup_expired_locks(self):
        """Clean up expired locks"""
        expired = [
            resource for resource, lock in self.locks.items()
            if lock.is_expired()
        ]
        
        for resource in expired:
            lock = self.locks[resource]
            logger.info(f"Lock expired: {lock.lock_id} on {resource}")
            del self.locks[resource]
    
    def get_locks(self) -> List[Dict[str, Any]]:
        """Get all active locks"""
        with self.lock:
            self._cleanup_expired_locks()
            return [
                {
                    "lock_id": lock.lock_id,
                    "resource": lock.resource,
                    "owner": lock.owner,
                    "age": time.time() - lock.acquired_at,
                    "ttl": lock.ttl
                }
                for lock in self.locks.values()
            ]


# ============================================================================
# 6. CLUSTER MANAGEMENT
# ============================================================================

@dataclass
class ClusterNode:
    """Cluster node information"""
    node_id: str
    host: str
    port: int
    status: str = "active"
    joined_at: float = field(default_factory=time.time)
    last_seen: float = field(default_factory=time.time)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def is_alive(self, timeout: float = 30.0) -> bool:
        """Check if node is alive"""
        return (time.time() - self.last_seen) < timeout


@dataclass
class ClusterInfo:
    """Cluster information"""
    cluster_id: str
    name: str
    nodes: Dict[str, ClusterNode] = field(default_factory=dict)
    created_at: float = field(default_factory=time.time)
    version: str = "1.0.0"


class ClusterManager:
    """
    Distributed cluster management
    
    Features:
    - Node membership management
    - Node health monitoring
    - Cluster topology tracking
    - Node metadata management
    - Graceful node removal
    """
    
    def __init__(self, cluster_id: str, cluster_name: str):
        self.cluster = ClusterInfo(
            cluster_id=cluster_id,
            name=cluster_name
        )
        self.lock = threading.Lock()
        logger.info(f"ClusterManager initialized: {cluster_name}")
    
    def join_node(
        self,
        node_id: str,
        host: str,
        port: int,
        metadata: Optional[Dict[str, Any]] = None
    ) -> bool:
        """Add a node to the cluster"""
        with self.lock:
            if node_id in self.cluster.nodes:
                logger.warning(f"Node {node_id} already in cluster")
                return False
            
            node = ClusterNode(
                node_id=node_id,
                host=host,
                port=port,
                metadata=metadata or {}
            )
            
            self.cluster.nodes[node_id] = node
            logger.info(f"Node {node_id} joined cluster at {host}:{port}")
            return True
    
    def leave_node(self, node_id: str) -> bool:
        """Remove a node from the cluster"""
        with self.lock:
            if node_id in self.cluster.nodes:
                del self.cluster.nodes[node_id]
                logger.info(f"Node {node_id} left cluster")
                return True
        return False
    
    def update_node_heartbeat(self, node_id: str):
        """Update node heartbeat"""
        with self.lock:
            if node_id in self.cluster.nodes:
                self.cluster.nodes[node_id].last_seen = time.time()
    
    def get_alive_nodes(self) -> List[ClusterNode]:
        """Get all alive nodes"""
        with self.lock:
            return [
                node for node in self.cluster.nodes.values()
                if node.is_alive()
            ]
    
    def get_cluster_size(self) -> int:
        """Get cluster size"""
        return len(self.get_alive_nodes())
    
    def get_cluster_info(self) -> Dict[str, Any]:
        """Get cluster information"""
        alive_nodes = self.get_alive_nodes()
        
        return {
            "cluster_id": self.cluster.cluster_id,
            "name": self.cluster.name,
            "version": self.cluster.version,
            "size": len(alive_nodes),
            "total_nodes": len(self.cluster.nodes),
            "nodes": [
                {
                    "node_id": node.node_id,
                    "host": node.host,
                    "port": node.port,
                    "status": node.status,
                    "uptime": time.time() - node.joined_at
                }
                for node in alive_nodes
            ]
        }


# ============================================================================
# 7. CONFIGURATION MANAGEMENT
# ============================================================================

@dataclass
class ConfigEntry:
    """Configuration entry"""
    key: str
    value: Any
    version: int = 1
    updated_at: float = field(default_factory=time.time)
    metadata: Dict[str, Any] = field(default_factory=dict)


class ConfigurationManager:
    """
    Distributed configuration management
    
    Features:
    - Centralized configuration storage
    - Configuration versioning
    - Watch notifications
    - Atomic updates
    - Configuration validation
    """
    
    def __init__(self):
        self.configs: Dict[str, ConfigEntry] = {}
        self.watchers: Dict[str, List[Callable]] = defaultdict(list)
        self.lock = threading.Lock()
        logger.info("ConfigurationManager initialized")
    
    def set(
        self,
        key: str,
        value: Any,
        metadata: Optional[Dict[str, Any]] = None
    ):
        """Set a configuration value"""
        with self.lock:
            if key in self.configs:
                entry = self.configs[key]
                entry.value = value
                entry.version += 1
                entry.updated_at = time.time()
                if metadata:
                    entry.metadata.update(metadata)
            else:
                entry = ConfigEntry(
                    key=key,
                    value=value,
                    metadata=metadata or {}
                )
                self.configs[key] = entry
            
            logger.info(f"Config set: {key} = {value} (v{entry.version})")
            
            # Notify watchers
            self._notify_watchers(key, value)
    
    def get(self, key: str, default: Any = None) -> Any:
        """Get a configuration value"""
        with self.lock:
            entry = self.configs.get(key)
            return entry.value if entry else default
    
    def delete(self, key: str) -> bool:
        """Delete a configuration"""
        with self.lock:
            if key in self.configs:
                del self.configs[key]
                logger.info(f"Config deleted: {key}")
                return True
        return False
    
    def watch(self, key: str, callback: Callable):
        """Watch a configuration key for changes"""
        with self.lock:
            self.watchers[key].append(callback)
            logger.info(f"Watcher added for config: {key}")
    
    def _notify_watchers(self, key: str, value: Any):
        """Notify watchers of configuration change"""
        callbacks = self.watchers.get(key, [])
        for callback in callbacks:
            try:
                callback(key, value)
            except Exception as e:
                logger.error(f"Watcher callback error: {e}")
    
    def get_all(self) -> Dict[str, Any]:
        """Get all configurations"""
        with self.lock:
            return {
                key: entry.value
                for key, entry in self.configs.items()
            }
    
    def get_with_metadata(self, key: str) -> Optional[Dict[str, Any]]:
        """Get configuration with metadata"""
        with self.lock:
            entry = self.configs.get(key)
            if entry:
                return {
                    "key": entry.key,
                    "value": entry.value,
                    "version": entry.version,
                    "updated_at": entry.updated_at,
                    "metadata": entry.metadata
                }
        return None


# ============================================================================
# 8. JOB SCHEDULING
# ============================================================================

@dataclass
class ScheduledJob:
    """Scheduled job"""
    job_id: str
    name: str
    func: Callable
    schedule_type: str  # "interval", "cron", "once"
    interval: Optional[float] = None  # seconds
    cron_expression: Optional[str] = None
    next_run: float = field(default_factory=time.time)
    last_run: Optional[float] = None
    enabled: bool = True
    metadata: Dict[str, Any] = field(default_factory=dict)


class JobScheduler:
    """
    Distributed job scheduler
    
    Features:
    - Interval-based scheduling
    - Cron-like scheduling
    - One-time jobs
    - Job priority
    - Distributed execution
    - Job persistence
    """
    
    def __init__(self):
        self.jobs: Dict[str, ScheduledJob] = {}
        self.job_queue: List[Tuple[float, str]] = []  # (next_run, job_id)
        self.execution_history: List[Dict[str, Any]] = []
        self.lock = threading.Lock()
        heapq.heapify(self.job_queue)
        logger.info("JobScheduler initialized")
    
    def schedule_interval(
        self,
        name: str,
        func: Callable,
        interval: float,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """Schedule a job to run at fixed intervals"""
        job_id = f"job-{uuid.uuid4().hex[:8]}"
        
        job = ScheduledJob(
            job_id=job_id,
            name=name,
            func=func,
            schedule_type="interval",
            interval=interval,
            metadata=metadata or {}
        )
        
        with self.lock:
            self.jobs[job_id] = job
            heapq.heappush(self.job_queue, (job.next_run, job_id))
        
        logger.info(f"Scheduled interval job: {name} every {interval}s")
        return job_id
    
    def schedule_once(
        self,
        name: str,
        func: Callable,
        delay: float = 0.0,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """Schedule a job to run once"""
        job_id = f"job-{uuid.uuid4().hex[:8]}"
        
        job = ScheduledJob(
            job_id=job_id,
            name=name,
            func=func,
            schedule_type="once",
            next_run=time.time() + delay,
            metadata=metadata or {}
        )
        
        with self.lock:
            self.jobs[job_id] = job
            heapq.heappush(self.job_queue, (job.next_run, job_id))
        
        logger.info(f"Scheduled one-time job: {name} in {delay}s")
        return job_id
    
    def unschedule(self, job_id: str) -> bool:
        """Unschedule a job"""
        with self.lock:
            if job_id in self.jobs:
                del self.jobs[job_id]
                logger.info(f"Unscheduled job: {job_id}")
                return True
        return False
    
    def run_pending(self) -> List[str]:
        """Run all pending jobs"""
        executed_jobs = []
        current_time = time.time()
        
        with self.lock:
            while self.job_queue and self.job_queue[0][0] <= current_time:
                next_run, job_id = heapq.heappop(self.job_queue)
                
                if job_id not in self.jobs:
                    continue
                
                job = self.jobs[job_id]
                
                if not job.enabled:
                    continue
                
                # Execute job
                self._execute_job(job)
                executed_jobs.append(job_id)
                
                # Reschedule if interval job
                if job.schedule_type == "interval" and job.interval:
                    job.next_run = current_time + job.interval
                    heapq.heappush(self.job_queue, (job.next_run, job_id))
                elif job.schedule_type == "once":
                    # Remove one-time jobs
                    del self.jobs[job_id]
        
        return executed_jobs
    
    def _execute_job(self, job: ScheduledJob):
        """Execute a scheduled job"""
        logger.info(f"Executing job: {job.name}")
        
        start_time = time.time()
        success = False
        error = None
        
        try:
            job.func()
            success = True
        except Exception as e:
            logger.error(f"Job execution failed: {e}")
            error = str(e)
        finally:
            duration = time.time() - start_time
            job.last_run = start_time
            
            # Record execution
            self.execution_history.append({
                "job_id": job.job_id,
                "name": job.name,
                "executed_at": start_time,
                "duration": duration,
                "success": success,
                "error": error
            })
    
    def get_scheduled_jobs(self) -> List[Dict[str, Any]]:
        """Get all scheduled jobs"""
        with self.lock:
            return [
                {
                    "job_id": job.job_id,
                    "name": job.name,
                    "schedule_type": job.schedule_type,
                    "interval": job.interval,
                    "next_run": job.next_run,
                    "last_run": job.last_run,
                    "enabled": job.enabled
                }
                for job in self.jobs.values()
            ]
    
    def get_execution_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """Get job execution history"""
        with self.lock:
            return self.execution_history[-limit:]


# ============================================================================
# DEMONSTRATION
# ============================================================================

def demo_distributed_coordination():
    """Demonstrate distributed coordination & orchestration"""
    
    print("=" * 80)
    print("DISTRIBUTED SYSTEM COORDINATION & ORCHESTRATION")
    print("=" * 80)
    print()
    print("üèóÔ∏è  COMPONENTS:")
    print("   1. Service Discovery & Registration")
    print("   2. Distributed Consensus (Raft)")
    print("   3. Workflow Orchestration (DAG)")
    print("   4. Leader Election")
    print("   5. Distributed Locking")
    print("   6. Cluster Management")
    print("   7. Configuration Management")
    print("   8. Job Scheduling")
    print()
    
    # ========================================================================
    # 1. SERVICE DISCOVERY
    # ========================================================================
    print("=" * 80)
    print("1. SERVICE DISCOVERY & REGISTRATION")
    print("=" * 80)
    
    registry = ServiceRegistry()
    
    # Register services
    print("\nüîç Registering services...")
    services = []
    for i in range(3):
        service_id = registry.register(
            service_name="nutrition-api",
            host=f"10.0.0.{i+1}",
            port=8000 + i,
            tags=["api", "production"],
            metadata={"region": "us-east" if i < 2 else "us-west"}
        )
        services.append(service_id)
        registry.heartbeat(service_id)
    
    print(f"   ‚úÖ Registered {len(services)} service instances")
    
    # Discover services
    print("\nüîç Discovering services...")
    instances = registry.discover("nutrition-api")
    print(f"   Found {len(instances)} healthy instances:")
    for inst in instances:
        print(f"      - {inst.service_id}: {inst.host}:{inst.port}")
    
    # Load balancing
    print("\n‚öñÔ∏è  Load balancing...")
    for i in range(5):
        instance = registry.get_instance("nutrition-api", strategy="round_robin")
        if instance:
            print(f"   Request {i+1} ‚Üí {instance.host}:{instance.port}")
    
    all_services = registry.get_all_services()
    print(f"\nüìä Total registered services: {len(all_services)}")
    
    # ========================================================================
    # 2. DISTRIBUTED CONSENSUS (RAFT)
    # ========================================================================
    print("\n" + "=" * 80)
    print("2. DISTRIBUTED CONSENSUS (Raft Algorithm)")
    print("=" * 80)
    
    cluster_nodes = ["node-2", "node-3", "node-4"]
    raft = RaftConsensus("node-1", cluster_nodes)
    
    # Election
    print("\nüó≥Ô∏è  Starting leader election...")
    raft.start_election()
    
    # Simulate votes
    for i in range(2):
        term, granted = raft.request_vote(
            term=1,
            candidate_id="node-1",
            last_log_index=0,
            last_log_term=0
        )
        if granted:
            raft.receive_vote(term, granted)
    
    state = raft.get_state()
    print(f"\nüìä Consensus State:")
    print(f"   Node: {state['node_id']}")
    print(f"   State: {state['state']}")
    print(f"   Term: {state['term']}")
    print(f"   Log entries: {state['log_length']}")
    
    # Log replication
    print("\nüìù Replicating commands...")
    commands = [
        {"op": "set", "key": "calories", "value": 2000},
        {"op": "set", "key": "protein", "value": 150},
        {"op": "set", "key": "carbs", "value": 250}
    ]
    
    for cmd in commands:
        raft.replicate_log(cmd)
    
    state = raft.get_state()
    print(f"   ‚úÖ Replicated {len(commands)} commands")
    print(f"   State machine: {state['state_machine']}")
    
    # ========================================================================
    # 3. WORKFLOW ORCHESTRATION
    # ========================================================================
    print("\n" + "=" * 80)
    print("3. WORKFLOW ORCHESTRATION (DAG)")
    print("=" * 80)
    
    orchestrator = WorkflowOrchestrator()
    
    # Create workflow
    print("\nüîÑ Creating ML training workflow...")
    workflow_id = orchestrator.create_workflow(
        name="ML Training Pipeline",
        metadata={"model": "nutrition-classifier"}
    )
    
    # Define tasks
    def task_load_data():
        time.sleep(0.1)
        return {"records": 10000}
    
    def task_preprocess():
        time.sleep(0.1)
        return {"features": 50}
    
    def task_train_model():
        time.sleep(0.2)
        return {"accuracy": 0.95}
    
    def task_evaluate():
        time.sleep(0.1)
        return {"f1_score": 0.93}
    
    # Add tasks with dependencies
    orchestrator.add_task(workflow_id, "load_data", "Load Data", task_load_data)
    orchestrator.add_task(
        workflow_id, "preprocess", "Preprocess",
        task_preprocess, dependencies=["load_data"]
    )
    orchestrator.add_task(
        workflow_id, "train", "Train Model",
        task_train_model, dependencies=["preprocess"]
    )
    orchestrator.add_task(
        workflow_id, "evaluate", "Evaluate",
        task_evaluate, dependencies=["train"]
    )
    
    print("   ‚úÖ Added 4 tasks with dependencies")
    
    # Execute workflow
    print("\n‚ñ∂Ô∏è  Executing workflow...")
    success = orchestrator.execute_workflow(workflow_id)
    
    status = orchestrator.get_workflow_status(workflow_id)
    if status:
        print(f"\n‚úÖ Workflow completed: {status['status']}")
        print(f"   Tasks:")
        for task_id, task_status in status['tasks'].items():
            print(f"      - {task_id}: {task_status}")
    
    # ========================================================================
    # 4. LEADER ELECTION
    # ========================================================================
    print("\n" + "=" * 80)
    print("4. LEADER ELECTION")
    print("=" * 80)
    
    cluster = ["node-2", "node-3", "node-4", "node-5"]
    election = LeaderElection("node-1", cluster)
    
    print("\nüó≥Ô∏è  Conducting leader election...")
    leader = election.elect_leader()
    print(f"   ‚úÖ Elected leader: {leader}")
    print(f"   Current node is leader: {election.is_leader}")
    
    # Heartbeat
    print("\nüíì Sending leader heartbeat...")
    election.send_heartbeat()
    is_healthy = election.check_leader_health()
    print(f"   Leader health: {'‚úÖ Healthy' if is_healthy else '‚ùå Unhealthy'}")
    
    # ========================================================================
    # 5. DISTRIBUTED LOCKING
    # ========================================================================
    print("\n" + "=" * 80)
    print("5. DISTRIBUTED LOCKING")
    print("=" * 80)
    
    lock_manager = DistributedLockManager()
    
    print("\nüîí Acquiring distributed locks...")
    locks = []
    resources = ["database", "cache", "storage"]
    
    for resource in resources:
        lock_id = lock_manager.acquire(
            resource=resource,
            owner="worker-1",
            ttl=30.0
        )
        if lock_id:
            locks.append((lock_id, resource))
    
    print(f"   ‚úÖ Acquired {len(locks)} locks")
    
    # Show active locks
    active_locks = lock_manager.get_locks()
    print(f"\nüìä Active locks: {len(active_locks)}")
    for lock in active_locks:
        print(f"   - {lock['resource']}: locked by {lock['owner']}")
    
    # Release locks
    print("\nüîì Releasing locks...")
    for lock_id, resource in locks:
        lock_manager.release(lock_id, "worker-1")
    
    active_locks = lock_manager.get_locks()
    print(f"   ‚úÖ Active locks remaining: {len(active_locks)}")
    
    # ========================================================================
    # 6. CLUSTER MANAGEMENT
    # ========================================================================
    print("\n" + "=" * 80)
    print("6. CLUSTER MANAGEMENT")
    print("=" * 80)
    
    cluster_mgr = ClusterManager("cluster-1", "Nutrition AI Cluster")
    
    print("\nüåê Building cluster...")
    nodes = [
        ("node-1", "10.0.1.1", 8000),
        ("node-2", "10.0.1.2", 8000),
        ("node-3", "10.0.1.3", 8000),
        ("node-4", "10.0.1.4", 8000)
    ]
    
    for node_id, host, port in nodes:
        cluster_mgr.join_node(
            node_id, host, port,
            metadata={"role": "worker", "region": "us-east"}
        )
        cluster_mgr.update_node_heartbeat(node_id)
    
    print(f"   ‚úÖ Cluster size: {cluster_mgr.get_cluster_size()} nodes")
    
    cluster_info = cluster_mgr.get_cluster_info()
    print(f"\nüìä Cluster Info:")
    print(f"   Name: {cluster_info['name']}")
    print(f"   Size: {cluster_info['size']} nodes")
    print(f"   Nodes:")
    for node in cluster_info['nodes']:
        print(f"      - {node['node_id']}: {node['host']}:{node['port']}")
    
    # ========================================================================
    # 7. CONFIGURATION MANAGEMENT
    # ========================================================================
    print("\n" + "=" * 80)
    print("7. CONFIGURATION MANAGEMENT")
    print("=" * 80)
    
    config_mgr = ConfigurationManager()
    
    print("\n‚öôÔ∏è  Setting configurations...")
    configs = {
        "ml.model.version": "v2.1.0",
        "ml.batch_size": 32,
        "api.rate_limit": 1000,
        "cache.ttl": 3600,
        "database.pool_size": 20
    }
    
    for key, value in configs.items():
        config_mgr.set(key, value)
    
    print(f"   ‚úÖ Set {len(configs)} configurations")
    
    # Watch configuration
    print("\nüëÅÔ∏è  Setting up configuration watcher...")
    watch_count = [0]
    
    def on_config_change(key, value):
        watch_count[0] += 1
        print(f"   üîî Config changed: {key} = {value}")
    
    config_mgr.watch("ml.batch_size", on_config_change)
    config_mgr.set("ml.batch_size", 64)
    
    # Get configurations
    print(f"\nüìä Active configurations: {len(config_mgr.get_all())}")
    sample_keys = ["ml.model.version", "api.rate_limit", "ml.batch_size"]
    for key in sample_keys:
        value = config_mgr.get(key)
        print(f"   {key} = {value}")
    
    # ========================================================================
    # 8. JOB SCHEDULING
    # ========================================================================
    print("\n" + "=" * 80)
    print("8. JOB SCHEDULING")
    print("=" * 80)
    
    scheduler = JobScheduler()
    
    print("\n‚è∞ Scheduling jobs...")
    
    # Interval jobs
    job_count = [0]
    
    def backup_job():
        job_count[0] += 1
    
    def cleanup_job():
        job_count[0] += 1
    
    def analytics_job():
        job_count[0] += 1
    
    job1 = scheduler.schedule_interval("Database Backup", backup_job, interval=3600)
    job2 = scheduler.schedule_interval("Cache Cleanup", cleanup_job, interval=1800)
    job3 = scheduler.schedule_once("Analytics Report", analytics_job, delay=0)
    
    print(f"   ‚úÖ Scheduled 3 jobs")
    
    # Run pending jobs
    print("\n‚ñ∂Ô∏è  Running pending jobs...")
    time.sleep(0.1)  # Ensure one-time job is ready
    executed = scheduler.run_pending()
    print(f"   ‚úÖ Executed {len(executed)} jobs")
    
    # Show scheduled jobs
    scheduled = scheduler.get_scheduled_jobs()
    print(f"\nüìä Scheduled jobs: {len(scheduled)}")
    for job in scheduled:
        next_run_in = job['next_run'] - time.time()
        print(f"   - {job['name']}: next run in {next_run_in:.1f}s")
    
    # ========================================================================
    # SUMMARY
    # ========================================================================
    print("\n" + "=" * 80)
    print("‚úÖ DISTRIBUTED COORDINATION COMPLETE")
    print("=" * 80)
    
    print("\nüì¶ CAPABILITIES:")
    print("   ‚úì Service discovery with health checking")
    print("   ‚úì Raft consensus with log replication")
    print("   ‚úì DAG-based workflow orchestration")
    print("   ‚úì Automatic leader election")
    print("   ‚úì Distributed locking with TTL")
    print("   ‚úì Cluster membership management")
    print("   ‚úì Centralized configuration management")
    print("   ‚úì Flexible job scheduling")
    
    print("\nüéØ COORDINATION METRICS:")
    print(f"   Services registered: {len(services)} ‚úì")
    print(f"   Raft log entries: {state['log_length']} ‚úì")
    print(f"   Workflow tasks: {status['task_count']} ‚úì")
    print(f"   Leader elected: {leader} ‚úì")
    print(f"   Locks managed: {len(locks)} ‚úì")
    print(f"   Cluster size: {cluster_mgr.get_cluster_size()} nodes ‚úì")
    print(f"   Configurations: {len(configs)} ‚úì")
    print(f"   Jobs scheduled: {len(scheduled)} ‚úì")
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    demo_distributed_coordination()
