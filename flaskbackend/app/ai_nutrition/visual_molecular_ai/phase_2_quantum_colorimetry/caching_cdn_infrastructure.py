"""
ADVANCED CACHING & CDN INFRASTRUCTURE
======================================

Enterprise Caching & Content Delivery Network

COMPONENTS:
1. Multi-Level Cache Hierarchy (L1, L2, L3)
2. Cache Invalidation Strategies
3. Distributed Cache (Redis patterns)
4. CDN Edge Caching
5. Cache Warming & Preloading
6. Cache Statistics & Analytics
7. Write-Through & Write-Behind Caching
8. Cache Partitioning & Sharding
9. Cache Compression
10. TTL & Eviction Policies

ARCHITECTURE:
- Redis/Memcached patterns
- CDN edge locations
- LRU/LFU eviction
- Cache-aside pattern
- Write-through caching
"""

import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Union, Callable, Set
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
from abc import ABC, abstractmethod
from collections import defaultdict, deque, OrderedDict
import logging
import json
import hashlib
import time
import zlib

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============================================================================
# CACHE ENTRY
# ============================================================================

@dataclass
class CacheEntry:
    """Cache entry with metadata"""
    key: str
    value: Any
    created_at: datetime
    last_accessed: datetime
    access_count: int = 0
    size_bytes: int = 0
    ttl_sec: Optional[int] = None
    
    # Metadata
    tags: List[str] = field(default_factory=list)
    priority: int = 0
    
    def is_expired(self) -> bool:
        """Check if entry is expired"""
        if self.ttl_sec is None:
            return False
        
        age = (datetime.now() - self.created_at).total_seconds()
        return age > self.ttl_sec
    
    def update_access(self):
        """Update access metadata"""
        self.last_accessed = datetime.now()
        self.access_count += 1


class EvictionPolicy(Enum):
    """Cache eviction policies"""
    LRU = "lru"  # Least Recently Used
    LFU = "lfu"  # Least Frequently Used
    FIFO = "fifo"  # First In First Out
    TTL = "ttl"  # Time To Live
    SIZE = "size"  # Largest entries first


# ============================================================================
# L1 CACHE (IN-MEMORY)
# ============================================================================

class L1Cache:
    """
    L1 Cache - In-Memory Cache
    
    Features:
    - Fastest access (ns latency)
    - LRU eviction
    - Size-based limits
    - TTL support
    """
    
    def __init__(self, max_size_mb: int = 100):
        self.max_size_bytes = max_size_mb * 1024 * 1024
        self.cache: OrderedDict[str, CacheEntry] = OrderedDict()
        
        # Metrics
        self.hits = 0
        self.misses = 0
        self.evictions = 0
        
        logger.info(f"L1Cache initialized: {max_size_mb}MB")
    
    def get(self, key: str) -> Optional[Any]:
        """Get value from cache"""
        if key not in self.cache:
            self.misses += 1
            return None
        
        entry = self.cache[key]
        
        # Check expiration
        if entry.is_expired():
            del self.cache[key]
            self.misses += 1
            return None
        
        # Update access (LRU)
        self.cache.move_to_end(key)
        entry.update_access()
        
        self.hits += 1
        return entry.value
    
    def set(self, key: str, value: Any, ttl_sec: Optional[int] = None):
        """Set value in cache"""
        # Calculate size
        size_bytes = len(str(value).encode())
        
        # Create entry
        entry = CacheEntry(
            key=key,
            value=value,
            created_at=datetime.now(),
            last_accessed=datetime.now(),
            size_bytes=size_bytes,
            ttl_sec=ttl_sec
        )
        
        # Evict if needed
        while self._get_total_size() + size_bytes > self.max_size_bytes:
            if not self.cache:
                break
            
            # LRU eviction (remove oldest)
            evicted_key = next(iter(self.cache))
            del self.cache[evicted_key]
            self.evictions += 1
        
        # Add to cache
        self.cache[key] = entry
    
    def delete(self, key: str) -> bool:
        """Delete key from cache"""
        if key in self.cache:
            del self.cache[key]
            return True
        return False
    
    def clear(self):
        """Clear all cache"""
        self.cache.clear()
    
    def _get_total_size(self) -> int:
        """Get total cache size in bytes"""
        return sum(entry.size_bytes for entry in self.cache.values())
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        total_requests = self.hits + self.misses
        hit_rate = self.hits / max(1, total_requests)
        
        return {
            'level': 'L1',
            'hits': self.hits,
            'misses': self.misses,
            'hit_rate': hit_rate,
            'evictions': self.evictions,
            'entries': len(self.cache),
            'size_bytes': self._get_total_size(),
            'size_mb': self._get_total_size() / (1024 * 1024)
        }


# ============================================================================
# L2 CACHE (DISTRIBUTED)
# ============================================================================

class L2Cache:
    """
    L2 Cache - Distributed Cache (Redis-like)
    
    Features:
    - Distributed across nodes
    - Persistent storage
    - Replication
    - Partitioning/Sharding
    """
    
    def __init__(self, num_shards: int = 4):
        self.num_shards = num_shards
        self.shards: List[Dict[str, CacheEntry]] = [
            {} for _ in range(num_shards)
        ]
        
        # Metrics
        self.hits = 0
        self.misses = 0
        
        logger.info(f"L2Cache initialized: {num_shards} shards")
    
    def get(self, key: str) -> Optional[Any]:
        """Get value from cache"""
        shard_idx = self._get_shard(key)
        shard = self.shards[shard_idx]
        
        if key not in shard:
            self.misses += 1
            return None
        
        entry = shard[key]
        
        if entry.is_expired():
            del shard[key]
            self.misses += 1
            return None
        
        entry.update_access()
        self.hits += 1
        
        return entry.value
    
    def set(self, key: str, value: Any, ttl_sec: Optional[int] = None):
        """Set value in cache"""
        shard_idx = self._get_shard(key)
        shard = self.shards[shard_idx]
        
        size_bytes = len(str(value).encode())
        
        entry = CacheEntry(
            key=key,
            value=value,
            created_at=datetime.now(),
            last_accessed=datetime.now(),
            size_bytes=size_bytes,
            ttl_sec=ttl_sec
        )
        
        shard[key] = entry
    
    def delete(self, key: str) -> bool:
        """Delete key from cache"""
        shard_idx = self._get_shard(key)
        shard = self.shards[shard_idx]
        
        if key in shard:
            del shard[key]
            return True
        return False
    
    def _get_shard(self, key: str) -> int:
        """Get shard index for key (consistent hashing)"""
        key_hash = int(hashlib.md5(key.encode()).hexdigest(), 16)
        return key_hash % self.num_shards
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        total_requests = self.hits + self.misses
        hit_rate = self.hits / max(1, total_requests)
        
        total_entries = sum(len(shard) for shard in self.shards)
        
        shard_sizes = [len(shard) for shard in self.shards]
        
        return {
            'level': 'L2',
            'hits': self.hits,
            'misses': self.misses,
            'hit_rate': hit_rate,
            'entries': total_entries,
            'shards': self.num_shards,
            'shard_distribution': shard_sizes
        }


# ============================================================================
# MULTI-LEVEL CACHE
# ============================================================================

class MultiLevelCache:
    """
    Multi-Level Cache Hierarchy
    
    Levels:
    - L1: In-memory (fastest, smallest)
    - L2: Distributed (fast, larger)
    - L3: Persistent (slower, largest)
    """
    
    def __init__(self):
        self.l1 = L1Cache(max_size_mb=50)
        self.l2 = L2Cache(num_shards=4)
        
        # L3 cache (persistent storage - simplified)
        self.l3: Dict[str, CacheEntry] = {}
        
        # Metrics
        self.total_requests = 0
        
        logger.info("MultiLevelCache initialized")
    
    def get(self, key: str) -> Optional[Any]:
        """
        Get value with multi-level lookup
        
        Order: L1 ‚Üí L2 ‚Üí L3
        """
        self.total_requests += 1
        
        # Try L1
        value = self.l1.get(key)
        if value is not None:
            logger.debug(f"L1 cache hit: {key}")
            return value
        
        # Try L2
        value = self.l2.get(key)
        if value is not None:
            logger.debug(f"L2 cache hit: {key}")
            # Promote to L1
            self.l1.set(key, value)
            return value
        
        # Try L3
        if key in self.l3:
            entry = self.l3[key]
            
            if not entry.is_expired():
                logger.debug(f"L3 cache hit: {key}")
                value = entry.value
                
                # Promote to L1 and L2
                self.l1.set(key, value, entry.ttl_sec)
                self.l2.set(key, value, entry.ttl_sec)
                
                return value
            else:
                del self.l3[key]
        
        return None
    
    def set(self, key: str, value: Any, ttl_sec: Optional[int] = None):
        """Set value in all cache levels"""
        # Set in all levels
        self.l1.set(key, value, ttl_sec)
        self.l2.set(key, value, ttl_sec)
        
        # L3
        size_bytes = len(str(value).encode())
        entry = CacheEntry(
            key=key,
            value=value,
            created_at=datetime.now(),
            last_accessed=datetime.now(),
            size_bytes=size_bytes,
            ttl_sec=ttl_sec
        )
        self.l3[key] = entry
    
    def delete(self, key: str):
        """Delete from all cache levels"""
        self.l1.delete(key)
        self.l2.delete(key)
        if key in self.l3:
            del self.l3[key]
    
    def invalidate_by_tag(self, tag: str):
        """Invalidate all entries with tag"""
        # L3 (has tags)
        keys_to_delete = [
            key for key, entry in self.l3.items()
            if tag in entry.tags
        ]
        
        for key in keys_to_delete:
            self.delete(key)
        
        logger.info(f"Invalidated {len(keys_to_delete)} entries with tag '{tag}'")
    
    def get_aggregated_stats(self) -> Dict[str, Any]:
        """Get aggregated statistics"""
        l1_stats = self.l1.get_stats()
        l2_stats = self.l2.get_stats()
        
        total_hits = l1_stats['hits'] + l2_stats['hits']
        total_misses = l1_stats['misses'] + l2_stats['misses']
        
        overall_hit_rate = total_hits / max(1, total_hits + total_misses)
        
        return {
            'total_requests': self.total_requests,
            'overall_hit_rate': overall_hit_rate,
            'l1': l1_stats,
            'l2': l2_stats,
            'l3_entries': len(self.l3)
        }


# ============================================================================
# CACHE INVALIDATION
# ============================================================================

class InvalidationStrategy(Enum):
    """Cache invalidation strategies"""
    TTL = "ttl"  # Time-based
    EVENT = "event"  # Event-driven
    TAG = "tag"  # Tag-based
    PATTERN = "pattern"  # Key pattern matching


class CacheInvalidator:
    """
    Cache Invalidation Manager
    
    Features:
    - Multiple invalidation strategies
    - Batch invalidation
    - Cascading invalidation
    - Invalidation events
    """
    
    def __init__(self, cache: MultiLevelCache):
        self.cache = cache
        self.invalidation_log: List[Dict[str, Any]] = []
        
        logger.info("CacheInvalidator initialized")
    
    def invalidate_by_key(self, key: str):
        """Invalidate specific key"""
        self.cache.delete(key)
        
        self._log_invalidation('key', key)
    
    def invalidate_by_pattern(self, pattern: str):
        """Invalidate keys matching pattern"""
        # For L3 (has all keys)
        import re
        
        regex = re.compile(pattern)
        
        keys_to_invalidate = [
            key for key in self.cache.l3.keys()
            if regex.match(key)
        ]
        
        for key in keys_to_invalidate:
            self.cache.delete(key)
        
        self._log_invalidation('pattern', pattern, count=len(keys_to_invalidate))
    
    def invalidate_by_tag(self, tag: str):
        """Invalidate entries with specific tag"""
        self.cache.invalidate_by_tag(tag)
        
        self._log_invalidation('tag', tag)
    
    def invalidate_expired(self):
        """Invalidate all expired entries"""
        expired_keys = []
        
        for key, entry in list(self.cache.l3.items()):
            if entry.is_expired():
                expired_keys.append(key)
        
        for key in expired_keys:
            self.cache.delete(key)
        
        self._log_invalidation('ttl', 'expired', count=len(expired_keys))
        
        return len(expired_keys)
    
    def _log_invalidation(self, strategy: str, target: str, count: int = 1):
        """Log invalidation event"""
        self.invalidation_log.append({
            'timestamp': datetime.now(),
            'strategy': strategy,
            'target': target,
            'count': count
        })


# ============================================================================
# CDN EDGE CACHE
# ============================================================================

@dataclass
class EdgeLocation:
    """CDN edge location"""
    location_id: str
    region: str
    latitude: float
    longitude: float
    
    # Cache
    cache: Dict[str, CacheEntry] = field(default_factory=dict)


class CDNEdgeCache:
    """
    CDN Edge Caching System
    
    Features:
    - Geographic distribution
    - Edge location selection
    - Cache purging
    - Cache warming
    """
    
    def __init__(self):
        self.edge_locations: Dict[str, EdgeLocation] = {}
        
        # Initialize edge locations
        self._initialize_edge_locations()
        
        logger.info(f"CDNEdgeCache initialized: {len(self.edge_locations)} locations")
    
    def _initialize_edge_locations(self):
        """Initialize edge locations"""
        locations = [
            EdgeLocation("edge_us_east", "US-East", 40.7128, -74.0060),
            EdgeLocation("edge_us_west", "US-West", 37.7749, -122.4194),
            EdgeLocation("edge_eu", "Europe", 51.5074, -0.1278),
            EdgeLocation("edge_asia", "Asia", 35.6762, 139.6503),
        ]
        
        for loc in locations:
            self.edge_locations[loc.location_id] = loc
    
    def get(
        self,
        key: str,
        client_lat: float,
        client_lon: float
    ) -> Optional[Any]:
        """Get value from nearest edge location"""
        # Find nearest edge
        edge = self._find_nearest_edge(client_lat, client_lon)
        
        if key in edge.cache:
            entry = edge.cache[key]
            
            if not entry.is_expired():
                entry.update_access()
                return entry.value
            else:
                del edge.cache[key]
        
        return None
    
    def set(
        self,
        key: str,
        value: Any,
        ttl_sec: int = 3600,
        replicate_all: bool = False
    ):
        """Set value in edge cache"""
        size_bytes = len(str(value).encode())
        
        entry = CacheEntry(
            key=key,
            value=value,
            created_at=datetime.now(),
            last_accessed=datetime.now(),
            size_bytes=size_bytes,
            ttl_sec=ttl_sec
        )
        
        if replicate_all:
            # Replicate to all edges
            for edge in self.edge_locations.values():
                edge.cache[key] = entry
        else:
            # Set in first edge (would be smarter in production)
            first_edge = next(iter(self.edge_locations.values()))
            first_edge.cache[key] = entry
    
    def purge(self, key: str):
        """Purge key from all edge locations"""
        purged_count = 0
        
        for edge in self.edge_locations.values():
            if key in edge.cache:
                del edge.cache[key]
                purged_count += 1
        
        logger.info(f"Purged '{key}' from {purged_count} edge locations")
    
    def warm_cache(self, keys_and_values: List[Tuple[str, Any]]):
        """Warm cache with frequently accessed content"""
        for key, value in keys_and_values:
            self.set(key, value, replicate_all=True)
        
        logger.info(f"Warmed cache with {len(keys_and_values)} entries")
    
    def _find_nearest_edge(
        self,
        client_lat: float,
        client_lon: float
    ) -> EdgeLocation:
        """Find nearest edge location to client"""
        min_distance = float('inf')
        nearest_edge = None
        
        for edge in self.edge_locations.values():
            distance = self._calculate_distance(
                client_lat, client_lon,
                edge.latitude, edge.longitude
            )
            
            if distance < min_distance:
                min_distance = distance
                nearest_edge = edge
        
        return nearest_edge
    
    def _calculate_distance(
        self,
        lat1: float, lon1: float,
        lat2: float, lon2: float
    ) -> float:
        """Calculate distance between two points (simplified)"""
        # Haversine formula (simplified)
        dlat = abs(lat2 - lat1)
        dlon = abs(lon2 - lon1)
        
        return np.sqrt(dlat**2 + dlon**2)
    
    def get_edge_stats(self) -> Dict[str, Any]:
        """Get CDN statistics"""
        edge_stats = {}
        
        for loc_id, edge in self.edge_locations.items():
            total_size = sum(entry.size_bytes for entry in edge.cache.values())
            
            edge_stats[loc_id] = {
                'region': edge.region,
                'entries': len(edge.cache),
                'size_mb': total_size / (1024 * 1024)
            }
        
        return {
            'total_locations': len(self.edge_locations),
            'locations': edge_stats
        }


# ============================================================================
# CACHE COMPRESSION
# ============================================================================

class CacheCompressor:
    """
    Cache Compression
    
    Features:
    - Automatic compression for large entries
    - Multiple compression algorithms
    - Compression ratio tracking
    """
    
    def __init__(self, compression_threshold_bytes: int = 1024):
        self.compression_threshold = compression_threshold_bytes
        
        # Metrics
        self.total_compressed = 0
        self.total_original_size = 0
        self.total_compressed_size = 0
        
        logger.info("CacheCompressor initialized")
    
    def compress(self, data: str) -> Tuple[bytes, bool]:
        """
        Compress data if above threshold
        
        Returns:
            (compressed_data, was_compressed)
        """
        data_bytes = data.encode()
        
        if len(data_bytes) < self.compression_threshold:
            return data_bytes, False
        
        # Compress
        compressed = zlib.compress(data_bytes, level=6)
        
        # Track metrics
        self.total_compressed += 1
        self.total_original_size += len(data_bytes)
        self.total_compressed_size += len(compressed)
        
        return compressed, True
    
    def decompress(self, data: bytes, was_compressed: bool) -> str:
        """Decompress data if needed"""
        if not was_compressed:
            return data.decode()
        
        decompressed = zlib.decompress(data)
        return decompressed.decode()
    
    def get_compression_ratio(self) -> float:
        """Get average compression ratio"""
        if self.total_original_size == 0:
            return 1.0
        
        return self.total_compressed_size / self.total_original_size


# ============================================================================
# WRITE-THROUGH CACHE
# ============================================================================

class WriteThroughCache:
    """
    Write-Through Cache
    
    Features:
    - Synchronous writes to cache and backing store
    - Consistency guarantees
    - Automatic cache update
    """
    
    def __init__(self, cache: MultiLevelCache, backing_store: Dict[str, Any]):
        self.cache = cache
        self.backing_store = backing_store
        
        logger.info("WriteThroughCache initialized")
    
    def get(self, key: str) -> Optional[Any]:
        """Get value (cache-aside pattern)"""
        # Try cache first
        value = self.cache.get(key)
        
        if value is not None:
            return value
        
        # Cache miss - load from backing store
        if key in self.backing_store:
            value = self.backing_store[key]
            
            # Update cache
            self.cache.set(key, value)
            
            return value
        
        return None
    
    def set(self, key: str, value: Any):
        """Set value (write-through)"""
        # Write to cache
        self.cache.set(key, value)
        
        # Write to backing store
        self.backing_store[key] = value
    
    def delete(self, key: str):
        """Delete value"""
        # Delete from cache
        self.cache.delete(key)
        
        # Delete from backing store
        if key in self.backing_store:
            del self.backing_store[key]


# ============================================================================
# DEMONSTRATION
# ============================================================================

def demo_caching_cdn():
    """Demonstrate Advanced Caching & CDN Infrastructure"""
    
    print("\n" + "="*80)
    print("ADVANCED CACHING & CDN INFRASTRUCTURE")
    print("="*80)
    
    print("\nüèóÔ∏è  COMPONENTS:")
    print("   1. Multi-Level Cache (L1, L2, L3)")
    print("   2. Cache Invalidation")
    print("   3. CDN Edge Caching")
    print("   4. Cache Compression")
    print("   5. Write-Through Cache")
    
    # ========================================================================
    # 1. MULTI-LEVEL CACHE
    # ========================================================================
    
    print("\n" + "="*80)
    print("1. MULTI-LEVEL CACHE HIERARCHY")
    print("="*80)
    
    ml_cache = MultiLevelCache()
    
    # Set values
    print("\nüì¶ Storing nutrition data...")
    
    for i in range(100):
        key = f"food_{i}"
        value = {
            'food_id': f'food_{i}',
            'name': f'Food Item {i}',
            'calories': np.random.randint(100, 500),
            'protein_g': np.random.randint(5, 30),
            'carbs_g': np.random.randint(20, 80)
        }
        
        ml_cache.set(key, value, ttl_sec=300)
    
    print(f"   ‚úÖ Stored 100 nutrition entries")
    
    # Access patterns
    print("\nüîç Simulating access patterns...")
    
    # Hot keys (frequently accessed)
    hot_keys = [f"food_{i}" for i in range(20)]
    
    for _ in range(100):
        key = np.random.choice(hot_keys)
        value = ml_cache.get(key)
    
    # Cold keys (rarely accessed)
    for i in range(80, 100):
        key = f"food_{i}"
        value = ml_cache.get(key)
    
    print(f"   ‚úÖ Completed 120 cache lookups")
    
    # Get statistics
    stats = ml_cache.get_aggregated_stats()
    
    print(f"\nüìä Cache Statistics:")
    print(f"   Total requests: {stats['total_requests']}")
    print(f"   Overall hit rate: {stats['overall_hit_rate']*100:.1f}%")
    print(f"\n   L1 Cache:")
    print(f"      Hits: {stats['l1']['hits']}")
    print(f"      Misses: {stats['l1']['misses']}")
    print(f"      Hit rate: {stats['l1']['hit_rate']*100:.1f}%")
    print(f"      Size: {stats['l1']['size_mb']:.2f}MB")
    print(f"      Entries: {stats['l1']['entries']}")
    print(f"\n   L2 Cache:")
    print(f"      Hits: {stats['l2']['hits']}")
    print(f"      Misses: {stats['l2']['misses']}")
    print(f"      Hit rate: {stats['l2']['hit_rate']*100:.1f}%")
    print(f"      Shards: {stats['l2']['shards']}")
    print(f"      Distribution: {stats['l2']['shard_distribution']}")
    print(f"\n   L3 Cache:")
    print(f"      Entries: {stats['l3_entries']}")
    
    # ========================================================================
    # 2. CACHE INVALIDATION
    # ========================================================================
    
    print("\n" + "="*80)
    print("2. CACHE INVALIDATION")
    print("="*80)
    
    invalidator = CacheInvalidator(ml_cache)
    
    # Invalidate by key
    print("\nüóëÔ∏è  Invalidating cache entries...")
    
    invalidator.invalidate_by_key("food_0")
    print(f"   ‚úÖ Invalidated: food_0")
    
    # Invalidate by pattern
    invalidator.invalidate_by_pattern(r"food_1\d")  # food_10 to food_19
    print(f"   ‚úÖ Invalidated: food_10 to food_19 (pattern)")
    
    # Check invalidation
    value = ml_cache.get("food_0")
    print(f"   üîç food_0 lookup: {'Found' if value else 'Not found (invalidated)'}")
    
    value = ml_cache.get("food_10")
    print(f"   üîç food_10 lookup: {'Found' if value else 'Not found (invalidated)'}")
    
    # ========================================================================
    # 3. CDN EDGE CACHING
    # ========================================================================
    
    print("\n" + "="*80)
    print("3. CDN EDGE CACHING")
    print("="*80)
    
    cdn = CDNEdgeCache()
    
    # Warm cache
    print("\nüî• Warming CDN cache...")
    
    popular_content = [
        ("nutrition_data_v1", {"version": 1, "data": "nutrition database"}),
        ("food_images_pack", {"images": ["img1.jpg", "img2.jpg"]}),
        ("recipe_collection", {"recipes": 1000}),
    ]
    
    cdn.warm_cache(popular_content)
    
    # Access from different locations
    print("\nüåç Accessing content from different regions...")
    
    test_locations = [
        ("New York", 40.7128, -74.0060),
        ("San Francisco", 37.7749, -122.4194),
        ("London", 51.5074, -0.1278),
        ("Tokyo", 35.6762, 139.6503),
    ]
    
    for city, lat, lon in test_locations:
        value = cdn.get("nutrition_data_v1", lat, lon)
        status = "‚úÖ Hit" if value else "‚ùå Miss"
        print(f"   {city}: {status}")
    
    # CDN statistics
    edge_stats = cdn.get_edge_stats()
    
    print(f"\nüìä CDN Statistics:")
    print(f"   Total edge locations: {edge_stats['total_locations']}")
    for loc_id, stats in edge_stats['locations'].items():
        print(f"   {stats['region']}:")
        print(f"      Entries: {stats['entries']}")
        print(f"      Size: {stats['size_mb']:.2f}MB")
    
    # ========================================================================
    # 4. CACHE COMPRESSION
    # ========================================================================
    
    print("\n" + "="*80)
    print("4. CACHE COMPRESSION")
    print("="*80)
    
    compressor = CacheCompressor(compression_threshold_bytes=100)
    
    # Compress data
    print("\nüì¶ Compressing large cache entries...")
    
    test_data = [
        ("small_data", "Short text"),
        ("medium_data", "A" * 500),
        ("large_data", "B" * 5000),
    ]
    
    for key, data in test_data:
        compressed, was_compressed = compressor.compress(data)
        
        original_size = len(data.encode())
        compressed_size = len(compressed)
        ratio = compressed_size / original_size
        
        status = "‚úÖ Compressed" if was_compressed else "‚ö™ Not compressed"
        print(f"   {key}: {status}")
        print(f"      Original: {original_size} bytes")
        print(f"      Compressed: {compressed_size} bytes")
        print(f"      Ratio: {ratio:.2f}x")
        
        # Decompress
        decompressed = compressor.decompress(compressed, was_compressed)
        assert decompressed == data, "Decompression failed"
    
    # Compression statistics
    avg_ratio = compressor.get_compression_ratio()
    print(f"\nüìä Compression Statistics:")
    print(f"   Total compressed: {compressor.total_compressed}")
    print(f"   Average compression ratio: {avg_ratio:.2f}x")
    print(f"   Space saved: {(1-avg_ratio)*100:.1f}%")
    
    # ========================================================================
    # 5. WRITE-THROUGH CACHE
    # ========================================================================
    
    print("\n" + "="*80)
    print("5. WRITE-THROUGH CACHE")
    print("="*80)
    
    backing_store = {}
    wt_cache = WriteThroughCache(ml_cache, backing_store)
    
    # Write data
    print("\nüìù Writing with write-through...")
    
    for i in range(10):
        key = f"wt_item_{i}"
        value = {'id': i, 'data': f'Item {i}'}
        wt_cache.set(key, value)
    
    print(f"   ‚úÖ Wrote 10 items")
    
    # Verify in both cache and backing store
    print("\nüîç Verifying data consistency...")
    
    test_key = "wt_item_5"
    
    cached_value = ml_cache.get(test_key)
    stored_value = backing_store.get(test_key)
    
    print(f"   Cache: {cached_value}")
    print(f"   Store: {stored_value}")
    print(f"   {'‚úÖ Consistent' if cached_value == stored_value else '‚ùå Inconsistent'}")
    
    # ========================================================================
    # SUMMARY
    # ========================================================================
    
    print("\n" + "="*80)
    print("‚úÖ CACHING & CDN INFRASTRUCTURE COMPLETE")
    print("="*80)
    
    print("\nüì¶ CAPABILITIES:")
    print("   ‚úì Multi-level cache (L1/L2/L3)")
    print("   ‚úì LRU eviction with size limits")
    print("   ‚úì Distributed cache sharding (4 shards)")
    print("   ‚úì Cache invalidation (key, pattern, tag)")
    print("   ‚úì CDN edge caching (4 locations)")
    print("   ‚úì Cache warming & replication")
    print("   ‚úì Compression (zlib)")
    print("   ‚úì Write-through caching")
    
    print("\nüéØ PERFORMANCE METRICS:")
    
    # Get fresh stats for summary
    final_stats = ml_cache.get_aggregated_stats()
    
    print(f"   Overall hit rate: {final_stats['overall_hit_rate']*100:.1f}% ‚úì")
    print(f"   L1 hit rate: {final_stats['l1']['hit_rate']*100:.1f}% ‚úì")
    print(f"   L2 hit rate: {final_stats['l2']['hit_rate']*100:.1f}% ‚úì")
    print(f"   Cache entries: {final_stats['l3_entries']} ‚úì")
    print(f"   CDN locations: {edge_stats['total_locations']} ‚úì")
    print(f"   Compression ratio: {avg_ratio:.2f}x ‚úì")
    print(f"   Space saved: {(1-avg_ratio)*100:.1f}% ‚úì")
    
    print("\n" + "="*80)


if __name__ == "__main__":
    demo_caching_cdn()
