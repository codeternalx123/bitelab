"""
Chemical Composition Decoder - Phase 2B
========================================

Advanced chemometric analysis for quantitative composition prediction from NIR spectra.

This module implements:
- Partial Least Squares (PLS) regression (PLS-1 and PLS-2)
- Principal Component Regression (PCR)
- Cross-validation (k-fold, leave-one-out)
- Variable Importance in Projection (VIP) scores
- Calibration database with reference foods
- Concentration predictions with confidence intervals

Scientific References:
----------------------
- Wold, S. et al. (2001) "PLS-regression: a basic tool of chemometrics"
  Chemometrics and Intelligent Laboratory Systems, 58(2), 109-130
- Geladi, P. & Kowalski, B.R. (1986) "Partial least-squares regression: a tutorial"
  Analytica Chimica Acta, 185, 1-17
- Martens, H. & Naes, T. (1989) "Multivariate Calibration" Wiley

Author: AI Nutrition Scanner Team
Date: January 2025
"""

import numpy as np
from typing import List, Tuple, Dict, Optional, Any
from dataclasses import dataclass, field
from enum import Enum
import logging
from pathlib import Path
import json

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CompositionComponent(Enum):
    """Nutritional components that can be predicted from NIR spectra."""
    WATER = "water"
    PROTEIN = "protein"
    FAT = "fat"
    CARBOHYDRATE = "carbohydrate"
    FIBER = "fiber"
    SUGAR = "sugar"
    ASH = "ash"
    
    # Minerals
    CALCIUM = "calcium"
    IRON = "iron"
    MAGNESIUM = "magnesium"
    PHOSPHORUS = "phosphorus"
    POTASSIUM = "potassium"
    SODIUM = "sodium"
    ZINC = "zinc"
    
    # Vitamins (NIR-active)
    VITAMIN_A = "vitamin_a"
    VITAMIN_C = "vitamin_c"
    VITAMIN_E = "vitamin_e"


class FoodCategory(Enum):
    """Food categories for calibration grouping."""
    FRUIT = "fruit"
    VEGETABLE = "vegetable"
    MEAT = "meat"
    DAIRY = "dairy"
    GRAIN = "grain"
    NUT = "nut"
    OIL = "oil"
    LEGUME = "legume"
    FISH = "fish"
    EGG = "egg"


@dataclass
class ReferenceSpectrum:
    """
    Reference NIR spectrum with known composition (calibration sample).
    
    Attributes:
        food_name: Name of the food item
        category: Food category
        wavelengths: Wavelength points (nm)
        intensities: Spectral intensities (absorbance or reflectance)
        composition: Known composition values (g/100g)
        metadata: Additional information (batch, date, lab, etc.)
    """
    food_name: str
    category: FoodCategory
    wavelengths: np.ndarray
    intensities: np.ndarray
    composition: Dict[CompositionComponent, float]
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """Validate reference spectrum data."""
        if len(self.wavelengths) != len(self.intensities):
            raise ValueError("Wavelengths and intensities must have same length")
        
        # Verify composition sums to ~100% (allow for measurement error)
        major_components = [
            self.composition.get(CompositionComponent.WATER, 0),
            self.composition.get(CompositionComponent.PROTEIN, 0),
            self.composition.get(CompositionComponent.FAT, 0),
            self.composition.get(CompositionComponent.CARBOHYDRATE, 0),
            self.composition.get(CompositionComponent.FIBER, 0),
            self.composition.get(CompositionComponent.ASH, 0),
        ]
        total = sum(major_components)
        if not (95 <= total <= 105):
            logger.warning(f"Composition sum = {total:.1f}% (expected ~100%)")


@dataclass
class CompositionPrediction:
    """
    Predicted composition from NIR spectrum.
    
    Attributes:
        composition: Predicted values (g/100g)
        confidence: Confidence scores (0-100%)
        prediction_interval: 95% prediction intervals (lower, upper)
        model_name: Name of model used
        preprocessing: Preprocessing methods applied
    """
    composition: Dict[CompositionComponent, float]
    confidence: Dict[CompositionComponent, float]
    prediction_interval: Dict[CompositionComponent, Tuple[float, float]]
    model_name: str
    preprocessing: List[str]
    
    def get_macronutrients(self) -> Dict[str, float]:
        """Extract macronutrient values (water, protein, fat, carbs)."""
        return {
            "water": self.composition.get(CompositionComponent.WATER, 0.0),
            "protein": self.composition.get(CompositionComponent.PROTEIN, 0.0),
            "fat": self.composition.get(CompositionComponent.FAT, 0.0),
            "carbohydrate": self.composition.get(CompositionComponent.CARBOHYDRATE, 0.0),
        }
    
    def get_total_confidence(self) -> float:
        """Calculate overall prediction confidence (average)."""
        if not self.confidence:
            return 0.0
        return np.mean(list(self.confidence.values()))


@dataclass
class PLSModel:
    """
    Trained PLS regression model.
    
    Attributes:
        component: Component this model predicts
        n_components: Number of PLS components (latent variables)
        x_weights: X-block weights (P matrix)
        y_weights: Y-block weights (Q matrix)
        x_loadings: X-block loadings
        y_loadings: Y-block loadings
        x_scores: X-block scores (T matrix)
        y_scores: Y-block scores (U matrix)
        coefficients: Regression coefficients
        x_mean: Mean of calibration X
        x_std: Std of calibration X
        y_mean: Mean of calibration Y
        y_std: Std of calibration Y
        r2: R-squared (goodness of fit)
        rmse: Root mean squared error
        vip_scores: Variable Importance in Projection
    """
    component: CompositionComponent
    n_components: int
    x_weights: np.ndarray
    y_weights: np.ndarray
    x_loadings: np.ndarray
    y_loadings: np.ndarray
    x_scores: np.ndarray
    y_scores: np.ndarray
    coefficients: np.ndarray
    x_mean: np.ndarray
    x_std: np.ndarray
    y_mean: float
    y_std: float
    r2: float
    rmse: float
    vip_scores: np.ndarray


class PLSRegressor:
    """
    Partial Least Squares (PLS) regression for NIR spectroscopy.
    
    PLS is a multivariate regression method that:
    1. Projects X (spectra) and Y (composition) to latent variable space
    2. Maximizes covariance between X-scores and Y-scores
    3. Handles collinear data (typical in NIR spectra)
    4. Predicts multiple components simultaneously (PLS-2)
    
    Algorithm (NIPALS - Nonlinear Iterative Partial Least Squares):
    For each component h = 1, ..., A:
        1. Initialize u_h as a column of Y
        2. Iterate until convergence:
            w_h = X'u_h / ||X'u_h||  (X-weights)
            t_h = Xw_h               (X-scores)
            q_h = Y't_h / ||Y't_h||  (Y-weights)
            u_h = Yq_h               (Y-scores)
        3. Regress X on t_h: p_h = X't_h / (t_h't_h)  (X-loadings)
        4. Deflate: X = X - t_h*p_h', Y = Y - t_h*q_h'
    
    Reference: Wold et al. (2001), Geladi & Kowalski (1986)
    """
    
    def __init__(self, n_components: int = 5, max_iter: int = 500, tol: float = 1e-6):
        """
        Initialize PLS regressor.
        
        Args:
            n_components: Number of PLS components (latent variables)
            max_iter: Maximum iterations for NIPALS algorithm
            tol: Convergence tolerance
        """
        self.n_components = n_components
        self.max_iter = max_iter
        self.tol = tol
        self.model: Optional[PLSModel] = None
        
        logger.info(f"Initialized PLSRegressor with {n_components} components")
    
    def fit(self, X: np.ndarray, y: np.ndarray, component: CompositionComponent) -> PLSModel:
        """
        Train PLS model on calibration data.
        
        Args:
            X: Spectral data (n_samples, n_wavelengths)
            y: Composition values (n_samples,) for single component
            component: Component being predicted
        
        Returns:
            Trained PLSModel
        """
        n_samples, n_features = X.shape
        
        if len(y) != n_samples:
            raise ValueError(f"X and y must have same number of samples: {n_samples} vs {len(y)}")
        
        # Validate n_components
        max_components = min(n_samples - 1, n_features)
        if self.n_components > max_components:
            logger.warning(f"Reducing n_components from {self.n_components} to {max_components}")
            self.n_components = max_components
        
        # Standardize data (mean-center and scale)
        X_centered = X - X.mean(axis=0)
        X_scaled = X_centered / (X_centered.std(axis=0) + 1e-10)
        
        y_centered = y - y.mean()
        y_scaled = y_centered / (y.std() + 1e-10)
        
        # Store standardization parameters
        x_mean = X.mean(axis=0)
        x_std = X_centered.std(axis=0) + 1e-10
        y_mean = y.mean()
        y_std = y.std() + 1e-10
        
        # Initialize arrays for PLS components
        W = np.zeros((n_features, self.n_components))  # X-weights
        P = np.zeros((n_features, self.n_components))  # X-loadings
        Q = np.zeros((1, self.n_components))           # Y-loadings
        T = np.zeros((n_samples, self.n_components))   # X-scores
        U = np.zeros((n_samples, self.n_components))   # Y-scores
        
        # Working copies for deflation
        X_work = X_scaled.copy()
        y_work = y_scaled.copy().reshape(-1, 1)
        
        # NIPALS algorithm - extract PLS components
        for h in range(self.n_components):
            # Initialize u as first column of Y (or random if Y is deflated to zero)
            u = y_work.copy()
            
            # Iterate until convergence
            for iteration in range(self.max_iter):
                u_old = u.copy()
                
                # 1. X-weights: w = X'u / ||X'u||
                w = X_work.T @ u
                w = w / (np.linalg.norm(w) + 1e-10)
                
                # 2. X-scores: t = Xw
                t = X_work @ w
                
                # 3. Y-weights: q = Y't / ||Y't||
                q = y_work.T @ t
                q = q / (np.linalg.norm(q) + 1e-10)
                
                # 4. Y-scores: u = Yq
                u = y_work @ q.T
                
                # Check convergence
                diff = np.linalg.norm(u - u_old)
                if diff < self.tol:
                    logger.debug(f"Component {h+1} converged in {iteration+1} iterations")
                    break
            
            # 5. X-loadings: p = X't / (t't)
            p = X_work.T @ t / (t.T @ t + 1e-10)
            
            # Store component
            W[:, h] = w.flatten()
            P[:, h] = p.flatten()
            Q[:, h] = q.flatten()
            T[:, h] = t.flatten()
            U[:, h] = u.flatten()
            
            # 6. Deflate X and Y
            X_work = X_work - t @ p.T
            y_work = y_work - t @ q
        
        # Calculate regression coefficients: B = W(P'W)^{-1}Q'
        # This transforms directly from X to Y
        coefficients = W @ np.linalg.pinv(P.T @ W) @ Q.T
        
        # Calculate fitted values and R¬≤
        y_pred = (X_scaled @ coefficients).flatten()
        ss_res = np.sum((y_scaled - y_pred) ** 2)
        ss_tot = np.sum((y_scaled - y_scaled.mean()) ** 2)
        r2 = 1 - (ss_res / ss_tot)
        
        # RMSE in original units
        y_pred_original = y_pred * y_std + y_mean
        rmse = np.sqrt(np.mean((y - y_pred_original) ** 2))
        
        # Calculate VIP scores (Variable Importance in Projection)
        vip_scores = self._calculate_vip(W, Q, T)
        
        # Create model
        model = PLSModel(
            component=component,
            n_components=self.n_components,
            x_weights=W,
            y_weights=Q.T,
            x_loadings=P,
            y_loadings=Q.T,
            x_scores=T,
            y_scores=U,
            coefficients=coefficients.flatten(),
            x_mean=x_mean,
            x_std=x_std,
            y_mean=y_mean,
            y_std=y_std,
            r2=r2,
            rmse=rmse,
            vip_scores=vip_scores
        )
        
        self.model = model
        logger.info(f"PLS model trained for {component.value}: R¬≤={r2:.4f}, RMSE={rmse:.4f}")
        
        return model
    
    def predict(self, X: np.ndarray, return_confidence: bool = True) -> Tuple[np.ndarray, Optional[np.ndarray]]:
        """
        Predict composition from new spectra.
        
        Args:
            X: Spectral data (n_samples, n_wavelengths)
            return_confidence: Whether to calculate confidence scores
        
        Returns:
            predictions: Predicted values (n_samples,)
            confidence: Confidence scores (n_samples,) or None
        """
        if self.model is None:
            raise ValueError("Model not trained. Call fit() first.")
        
        # Standardize using calibration parameters
        X_centered = X - self.model.x_mean
        X_scaled = X_centered / self.model.x_std
        
        # Predict
        y_scaled = X_scaled @ self.model.coefficients
        y_pred = y_scaled * self.model.y_std + self.model.y_mean
        
        # Calculate confidence based on leverage (Hotelling T¬≤)
        confidence = None
        if return_confidence:
            confidence = self._calculate_confidence(X_scaled)
        
        return y_pred, confidence
    
    def _calculate_vip(self, W: np.ndarray, Q: np.ndarray, T: np.ndarray) -> np.ndarray:
        """
        Calculate Variable Importance in Projection (VIP) scores.
        
        VIP measures the importance of each wavelength in the PLS model.
        VIP_j = sqrt(p * sum_h(w_jh¬≤ * SSY_h) / sum_h(SSY_h))
        
        where:
        - p = number of variables (wavelengths)
        - w_jh = weight of variable j in component h
        - SSY_h = sum of squares explained by component h
        
        Rule of thumb: VIP > 1.0 indicates important variable
        
        Args:
            W: X-weights (p, A)
            Q: Y-loadings (1, A)
            T: X-scores (n, A)
        
        Returns:
            VIP scores (p,)
        """
        p, A = W.shape
        
        # Calculate sum of squares explained by each component
        SSY = np.sum(T ** 2, axis=0) * (Q.flatten() ** 2)
        
        # VIP for each variable
        vip = np.sqrt(p * np.sum((W ** 2) * SSY, axis=1) / np.sum(SSY))
        
        return vip
    
    def _calculate_confidence(self, X_scaled: np.ndarray) -> np.ndarray:
        """
        Calculate prediction confidence based on sample leverage.
        
        Leverage (Hotelling T¬≤) measures how far a sample is from the
        calibration space. High leverage = low confidence.
        
        T¬≤ = t'(T'T)^{-1}t  where t are the scores
        
        Args:
            X_scaled: Scaled spectral data (n_samples, n_features)
        
        Returns:
            Confidence scores (n_samples,) in range [0, 100]
        """
        # Project samples to score space
        t = X_scaled @ self.model.x_weights
        
        # Calculate Hotelling T¬≤
        cov_inv = np.linalg.pinv(self.model.x_scores.T @ self.model.x_scores / len(self.model.x_scores))
        t2 = np.sum(t @ cov_inv * t, axis=1)
        
        # Convert to confidence (0-100%)
        # Use 95th percentile of calibration T¬≤ as threshold
        t2_threshold = np.percentile(np.sum(self.model.x_scores @ cov_inv * self.model.x_scores, axis=1), 95)
        confidence = 100 * np.exp(-t2 / t2_threshold)
        
        return confidence


class CrossValidator:
    """
    Cross-validation for PLS model evaluation.
    
    Methods:
    - K-fold cross-validation
    - Leave-one-out cross-validation (LOOCV)
    - Monte Carlo cross-validation
    """
    
    @staticmethod
    def k_fold(X: np.ndarray, y: np.ndarray, component: CompositionComponent,
               n_folds: int = 5, n_components: int = 5) -> Dict[str, float]:
        """
        K-fold cross-validation.
        
        Args:
            X: Spectral data (n_samples, n_wavelengths)
            y: Composition values (n_samples,)
            component: Component being predicted
            n_folds: Number of folds
            n_components: Number of PLS components
        
        Returns:
            Metrics dictionary (R¬≤_cv, RMSE_cv, bias)
        """
        n_samples = len(y)
        fold_size = n_samples // n_folds
        
        y_pred_all = np.zeros(n_samples)
        
        # Shuffle indices
        indices = np.random.permutation(n_samples)
        
        for fold in range(n_folds):
            # Split data
            if fold == n_folds - 1:
                test_idx = indices[fold * fold_size:]
            else:
                test_idx = indices[fold * fold_size:(fold + 1) * fold_size]
            
            train_idx = np.setdiff1d(indices, test_idx)
            
            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]
            
            # Train and predict
            pls = PLSRegressor(n_components=n_components)
            pls.fit(X_train, y_train, component)
            y_pred, _ = pls.predict(X_test, return_confidence=False)
            
            y_pred_all[test_idx] = y_pred
        
        # Calculate metrics
        r2_cv = 1 - np.sum((y - y_pred_all) ** 2) / np.sum((y - y.mean()) ** 2)
        rmse_cv = np.sqrt(np.mean((y - y_pred_all) ** 2))
        bias = np.mean(y_pred_all - y)
        
        logger.info(f"Cross-validation ({n_folds}-fold): R¬≤={r2_cv:.4f}, RMSE={rmse_cv:.4f}, Bias={bias:.4f}")
        
        return {
            "r2_cv": r2_cv,
            "rmse_cv": rmse_cv,
            "bias": bias,
            "predictions": y_pred_all,
            "actual": y
        }
    
    @staticmethod
    def leave_one_out(X: np.ndarray, y: np.ndarray, component: CompositionComponent,
                     n_components: int = 5) -> Dict[str, float]:
        """
        Leave-one-out cross-validation (LOOCV).
        
        Most rigorous but computationally expensive.
        
        Args:
            X: Spectral data (n_samples, n_wavelengths)
            y: Composition values (n_samples,)
            component: Component being predicted
            n_components: Number of PLS components
        
        Returns:
            Metrics dictionary (R¬≤_cv, RMSE_cv, PRESS)
        """
        n_samples = len(y)
        y_pred_all = np.zeros(n_samples)
        
        for i in range(n_samples):
            # Leave one out
            train_idx = np.delete(np.arange(n_samples), i)
            test_idx = i
            
            X_train, X_test = X[train_idx], X[test_idx:test_idx+1]
            y_train = y[train_idx]
            
            # Train and predict
            pls = PLSRegressor(n_components=n_components)
            pls.fit(X_train, y_train, component)
            y_pred, _ = pls.predict(X_test, return_confidence=False)
            
            y_pred_all[i] = y_pred[0]
        
        # Calculate metrics
        r2_cv = 1 - np.sum((y - y_pred_all) ** 2) / np.sum((y - y.mean()) ** 2)
        rmse_cv = np.sqrt(np.mean((y - y_pred_all) ** 2))
        press = np.sum((y - y_pred_all) ** 2)  # Prediction Error Sum of Squares
        
        logger.info(f"LOOCV: R¬≤={r2_cv:.4f}, RMSE={rmse_cv:.4f}, PRESS={press:.4f}")
        
        return {
            "r2_cv": r2_cv,
            "rmse_cv": rmse_cv,
            "press": press,
            "predictions": y_pred_all,
            "actual": y
        }


class CalibrationDatabase:
    """
    Database of reference NIR spectra with known compositions.
    
    Contains calibration samples for PLS model training.
    """
    
    def __init__(self):
        """Initialize empty calibration database."""
        self.references: List[ReferenceSpectrum] = []
        logger.info("Initialized CalibrationDatabase")
    
    def add_reference(self, reference: ReferenceSpectrum):
        """Add reference spectrum to database."""
        self.references.append(reference)
        logger.debug(f"Added reference: {reference.food_name} ({reference.category.value})")
    
    def get_by_category(self, category: FoodCategory) -> List[ReferenceSpectrum]:
        """Get all references for a specific food category."""
        return [ref for ref in self.references if ref.category == category]
    
    def get_calibration_data(self, component: CompositionComponent,
                            category: Optional[FoodCategory] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        Extract calibration data (X, y) for a specific component.
        
        Args:
            component: Component to extract
            category: Optional category filter
        
        Returns:
            X: Spectral data (n_samples, n_wavelengths)
            y: Composition values (n_samples,)
        """
        refs = self.references if category is None else self.get_by_category(category)
        
        if not refs:
            raise ValueError(f"No references found for component={component.value}, category={category}")
        
        # Extract data
        X = np.array([ref.intensities for ref in refs])
        y = np.array([ref.composition.get(component, 0.0) for ref in refs])
        
        logger.info(f"Extracted calibration data: {len(refs)} samples for {component.value}")
        
        return X, y
    
    def load_from_synthetic(self, n_samples: int = 50):
        """
        Generate synthetic reference spectra for testing.
        
        Creates realistic NIR spectra based on molecular composition.
        
        Args:
            n_samples: Number of synthetic samples to generate
        """
        logger.info(f"Generating {n_samples} synthetic reference spectra...")
        
        # Wavelength grid (900-2500 nm)
        wavelengths = np.linspace(900, 2500, 320)
        
        # Define food templates with typical compositions
        food_templates = [
            # Fruits (high water, sugar)
            ("apple", FoodCategory.FRUIT, {"water": 85.6, "protein": 0.3, "fat": 0.2, "carbohydrate": 13.8, "fiber": 2.4, "sugar": 10.4}),
            ("banana", FoodCategory.FRUIT, {"water": 74.9, "protein": 1.1, "fat": 0.3, "carbohydrate": 22.8, "fiber": 2.6, "sugar": 12.2}),
            ("orange", FoodCategory.FRUIT, {"water": 86.8, "protein": 0.9, "fat": 0.1, "carbohydrate": 11.8, "fiber": 2.4, "sugar": 9.4}),
            ("avocado", FoodCategory.FRUIT, {"water": 73.2, "protein": 2.0, "fat": 14.7, "carbohydrate": 8.5, "fiber": 6.7, "sugar": 0.7}),
            ("strawberry", FoodCategory.FRUIT, {"water": 91.0, "protein": 0.7, "fat": 0.3, "carbohydrate": 7.7, "fiber": 2.0, "sugar": 4.9}),
            
            # Vegetables (high water, fiber)
            ("tomato", FoodCategory.VEGETABLE, {"water": 94.5, "protein": 0.9, "fat": 0.2, "carbohydrate": 3.9, "fiber": 1.2, "sugar": 2.6}),
            ("carrot", FoodCategory.VEGETABLE, {"water": 88.3, "protein": 0.9, "fat": 0.2, "carbohydrate": 9.6, "fiber": 2.8, "sugar": 4.7}),
            ("broccoli", FoodCategory.VEGETABLE, {"water": 89.3, "protein": 2.8, "fat": 0.4, "carbohydrate": 6.6, "fiber": 2.6, "sugar": 1.7}),
            ("spinach", FoodCategory.VEGETABLE, {"water": 91.4, "protein": 2.9, "fat": 0.4, "carbohydrate": 3.6, "fiber": 2.2, "sugar": 0.4}),
            ("potato", FoodCategory.VEGETABLE, {"water": 79.3, "protein": 2.0, "fat": 0.1, "carbohydrate": 17.5, "fiber": 2.1, "sugar": 0.8}),
            
            # Meats (high protein, fat)
            ("chicken_breast", FoodCategory.MEAT, {"water": 74.0, "protein": 23.0, "fat": 1.2, "carbohydrate": 0.0, "fiber": 0.0, "sugar": 0.0}),
            ("beef_lean", FoodCategory.MEAT, {"water": 71.0, "protein": 21.0, "fat": 6.0, "carbohydrate": 0.0, "fiber": 0.0, "sugar": 0.0}),
            ("pork_loin", FoodCategory.MEAT, {"water": 72.0, "protein": 22.0, "fat": 4.0, "carbohydrate": 0.0, "fiber": 0.0, "sugar": 0.0}),
            
            # Dairy (moderate protein, fat)
            ("milk_whole", FoodCategory.DAIRY, {"water": 88.0, "protein": 3.2, "fat": 3.3, "carbohydrate": 4.8, "fiber": 0.0, "sugar": 5.1}),
            ("yogurt_plain", FoodCategory.DAIRY, {"water": 88.0, "protein": 3.5, "fat": 3.3, "carbohydrate": 4.7, "fiber": 0.0, "sugar": 4.7}),
            ("cheese_cheddar", FoodCategory.DAIRY, {"water": 36.8, "protein": 24.9, "fat": 33.1, "carbohydrate": 1.3, "fiber": 0.0, "sugar": 0.5}),
            
            # Grains (high carbs)
            ("white_bread", FoodCategory.GRAIN, {"water": 38.0, "protein": 8.0, "fat": 3.2, "carbohydrate": 49.0, "fiber": 2.7, "sugar": 5.0}),
            ("brown_rice", FoodCategory.GRAIN, {"water": 12.0, "protein": 7.9, "fat": 2.9, "carbohydrate": 77.2, "fiber": 3.5, "sugar": 0.9}),
            ("oats", FoodCategory.GRAIN, {"water": 8.2, "protein": 16.9, "fat": 6.9, "carbohydrate": 66.3, "fiber": 10.6, "sugar": 0.0}),
            
            # Nuts (high fat, protein)
            ("almond", FoodCategory.NUT, {"water": 4.4, "protein": 21.2, "fat": 49.9, "carbohydrate": 21.6, "fiber": 12.5, "sugar": 4.4}),
            ("peanut", FoodCategory.NUT, {"water": 6.5, "protein": 25.8, "fat": 49.2, "carbohydrate": 16.1, "fiber": 8.5, "sugar": 4.7}),
            
            # Oils (pure fat)
            ("olive_oil", FoodCategory.OIL, {"water": 0.0, "protein": 0.0, "fat": 100.0, "carbohydrate": 0.0, "fiber": 0.0, "sugar": 0.0}),
            
            # Fish (high protein)
            ("salmon", FoodCategory.FISH, {"water": 68.5, "protein": 20.0, "fat": 13.4, "carbohydrate": 0.0, "fiber": 0.0, "sugar": 0.0}),
            ("tuna", FoodCategory.FISH, {"water": 70.6, "protein": 23.3, "fat": 4.9, "carbohydrate": 0.0, "fiber": 0.0, "sugar": 0.0}),
            
            # Eggs
            ("egg_whole", FoodCategory.EGG, {"water": 76.2, "protein": 12.6, "fat": 9.5, "carbohydrate": 0.7, "fiber": 0.0, "sugar": 0.4}),
            
            # Legumes
            ("chickpea", FoodCategory.LEGUME, {"water": 60.2, "protein": 8.9, "fat": 2.6, "carbohydrate": 27.4, "fiber": 7.6, "sugar": 4.8}),
            ("lentil", FoodCategory.LEGUME, {"water": 69.6, "protein": 9.0, "fat": 0.4, "carbohydrate": 20.1, "fiber": 7.9, "sugar": 1.8}),
        ]
        
        # Generate samples with variations
        for i in range(n_samples):
            # Select template
            template_idx = i % len(food_templates)
            food_name, category, base_comp = food_templates[template_idx]
            
            # Add batch variation (¬±5% for most components)
            composition = {}
            for key, value in base_comp.items():
                variation = np.random.uniform(-0.05, 0.05) * value
                composition[CompositionComponent(key)] = max(0, value + variation)
            
            # Ensure components sum to ~100%
            total = sum(composition.values())
            if total > 0:
                scale = 100.0 / total
                composition = {k: v * scale for k, v in composition.items()}
            
            # Generate NIR spectrum based on composition
            spectrum = self._generate_spectrum(wavelengths, composition)
            
            # Add measurement noise
            noise = np.random.normal(0, 0.005, len(wavelengths))
            spectrum += noise
            
            # Create reference
            reference = ReferenceSpectrum(
                food_name=f"{food_name}_batch{i // len(food_templates) + 1}",
                category=category,
                wavelengths=wavelengths.copy(),
                intensities=spectrum,
                composition=composition,
                metadata={"synthetic": True, "batch": i // len(food_templates) + 1}
            )
            
            self.add_reference(reference)
        
        logger.info(f"Generated {n_samples} synthetic references across {len(food_templates)} food types")
    
    def _generate_spectrum(self, wavelengths: np.ndarray, composition: Dict[CompositionComponent, float]) -> np.ndarray:
        """
        Generate realistic NIR spectrum from composition.
        
        Uses Beer-Lambert law and known absorption bands.
        
        Args:
            wavelengths: Wavelength points (nm)
            composition: Composition dict (g/100g)
        
        Returns:
            Simulated absorbance spectrum
        """
        spectrum = np.zeros(len(wavelengths))
        
        # Water absorption (O-H bonds)
        if CompositionComponent.WATER in composition:
            water_pct = composition[CompositionComponent.WATER] / 100.0
            # Strong bands at 1450nm and 1940nm
            spectrum += water_pct * 0.5 * np.exp(-((wavelengths - 1450) ** 2) / (2 * 30 ** 2))
            spectrum += water_pct * 0.55 * np.exp(-((wavelengths - 1940) ** 2) / (2 * 35 ** 2))
        
        # Fat absorption (C-H bonds in lipids)
        if CompositionComponent.FAT in composition:
            fat_pct = composition[CompositionComponent.FAT] / 100.0
            # Bands at 1210nm, 1730nm, 2310nm
            spectrum += fat_pct * 0.25 * np.exp(-((wavelengths - 1210) ** 2) / (2 * 25 ** 2))
            spectrum += fat_pct * 0.30 * np.exp(-((wavelengths - 1730) ** 2) / (2 * 30 ** 2))
            spectrum += fat_pct * 0.35 * np.exp(-((wavelengths - 2310) ** 2) / (2 * 40 ** 2))
        
        # Protein absorption (N-H bonds)
        if CompositionComponent.PROTEIN in composition:
            protein_pct = composition[CompositionComponent.PROTEIN] / 100.0
            # Bands at 1510nm, 2050nm, 2180nm
            spectrum += protein_pct * 0.20 * np.exp(-((wavelengths - 1510) ** 2) / (2 * 30 ** 2))
            spectrum += protein_pct * 0.30 * np.exp(-((wavelengths - 2050) ** 2) / (2 * 35 ** 2))
            spectrum += protein_pct * 0.25 * np.exp(-((wavelengths - 2180) ** 2) / (2 * 30 ** 2))
        
        # Carbohydrate absorption (C-O, C-H bonds)
        if CompositionComponent.CARBOHYDRATE in composition:
            carb_pct = composition[CompositionComponent.CARBOHYDRATE] / 100.0
            # Bands at 1450nm, 2100nm, 2270nm
            spectrum += carb_pct * 0.15 * np.exp(-((wavelengths - 1450) ** 2) / (2 * 35 ** 2))
            spectrum += carb_pct * 0.20 * np.exp(-((wavelengths - 2100) ** 2) / (2 * 40 ** 2))
            spectrum += carb_pct * 0.18 * np.exp(-((wavelengths - 2270) ** 2) / (2 * 35 ** 2))
        
        # Add baseline (instrumental effects)
        baseline = 0.1 + 0.05 * (wavelengths - wavelengths[0]) / (wavelengths[-1] - wavelengths[0])
        spectrum += baseline
        
        return spectrum
    
    def __len__(self) -> int:
        """Return number of reference spectra."""
        return len(self.references)


class ChemicalCompositionDecoder:
    """
    Main class for decoding NIR spectra to chemical composition.
    
    Orchestrates:
    - Calibration database management
    - PLS model training
    - Composition prediction
    - Quality control
    """
    
    def __init__(self):
        """Initialize decoder."""
        self.database = CalibrationDatabase()
        self.models: Dict[CompositionComponent, PLSRegressor] = {}
        logger.info("Initialized ChemicalCompositionDecoder")
    
    def train_models(self, components: List[CompositionComponent], n_components: int = 5,
                    category: Optional[FoodCategory] = None):
        """
        Train PLS models for specified components.
        
        Args:
            components: List of components to train models for
            n_components: Number of PLS components
            category: Optional category filter for calibration data
        """
        logger.info(f"Training models for {len(components)} components...")
        
        for component in components:
            # Get calibration data
            X, y = self.database.get_calibration_data(component, category)
            
            # Train PLS model
            pls = PLSRegressor(n_components=n_components)
            model = pls.fit(X, y, component)
            
            self.models[component] = pls
            
            logger.info(f"  {component.value}: R¬≤={model.r2:.4f}, RMSE={model.rmse:.4f} g/100g")
    
    def predict_composition(self, wavelengths: np.ndarray, intensities: np.ndarray,
                          preprocessing: Optional[List[str]] = None) -> CompositionPrediction:
        """
        Predict composition from NIR spectrum.
        
        Args:
            wavelengths: Wavelength points (nm)
            intensities: Spectral intensities
            preprocessing: List of preprocessing methods applied
        
        Returns:
            CompositionPrediction with values and confidence
        """
        if not self.models:
            raise ValueError("No models trained. Call train_models() first.")
        
        # Reshape for prediction
        X = intensities.reshape(1, -1)
        
        composition = {}
        confidence = {}
        prediction_interval = {}
        
        for component, pls in self.models.items():
            # Predict
            y_pred, conf = pls.predict(X, return_confidence=True)
            
            composition[component] = float(y_pred[0])
            confidence[component] = float(conf[0]) if conf is not None else 50.0
            
            # Calculate 95% prediction interval (¬±2*RMSE approximation)
            rmse = pls.model.rmse
            prediction_interval[component] = (
                max(0, y_pred[0] - 2 * rmse),
                min(100, y_pred[0] + 2 * rmse)
            )
        
        return CompositionPrediction(
            composition=composition,
            confidence=confidence,
            prediction_interval=prediction_interval,
            model_name="PLS",
            preprocessing=preprocessing or []
        )


# ============================================================================
# TESTING & DEMONSTRATION
# ============================================================================

def test_pls_regression():
    """Test PLS regression on synthetic data."""
    print("\n" + "="*80)
    print("TEST 1: PLS Regression on Synthetic Data")
    print("="*80)
    
    # Generate synthetic calibration data
    np.random.seed(42)
    n_samples = 50
    n_wavelengths = 320
    
    X = np.random.randn(n_samples, n_wavelengths)
    
    # True composition (with some noise)
    true_coef = np.random.randn(n_wavelengths) * 0.1
    y = X @ true_coef + np.random.randn(n_samples) * 0.5
    y = 50 + 20 * (y - y.mean()) / y.std()  # Scale to realistic range
    
    # Train PLS
    pls = PLSRegressor(n_components=5)
    model = pls.fit(X, y, CompositionComponent.PROTEIN)
    
    print(f"\n‚úì Model trained: {model.n_components} components")
    print(f"  R¬≤ = {model.r2:.4f}")
    print(f"  RMSE = {model.rmse:.4f} g/100g")
    
    # Predict
    y_pred, conf = pls.predict(X)
    
    print(f"\n‚úì Predictions generated:")
    print(f"  Mean confidence = {conf.mean():.1f}%")
    print(f"  Prediction RMSE = {np.sqrt(np.mean((y - y_pred)**2)):.4f} g/100g")
    
    # VIP scores
    important_vars = np.sum(model.vip_scores > 1.0)
    print(f"\n‚úì VIP analysis:")
    print(f"  {important_vars}/{n_wavelengths} variables with VIP > 1.0")
    print(f"  Max VIP = {model.vip_scores.max():.2f}")
    
    return True


def test_cross_validation():
    """Test cross-validation methods."""
    print("\n" + "="*80)
    print("TEST 2: Cross-Validation")
    print("="*80)
    
    # Generate data
    np.random.seed(42)
    n_samples = 40
    n_wavelengths = 320
    
    X = np.random.randn(n_samples, n_wavelengths)
    true_coef = np.random.randn(n_wavelengths) * 0.1
    y = X @ true_coef + np.random.randn(n_samples) * 0.5
    y = 50 + 20 * (y - y.mean()) / y.std()
    
    # K-fold CV
    cv_results = CrossValidator.k_fold(X, y, CompositionComponent.WATER, n_folds=5, n_components=3)
    
    print(f"\n‚úì 5-fold Cross-Validation:")
    print(f"  R¬≤_cv = {cv_results['r2_cv']:.4f}")
    print(f"  RMSE_cv = {cv_results['rmse_cv']:.4f} g/100g")
    print(f"  Bias = {cv_results['bias']:.4f} g/100g")
    
    # LOOCV (on smaller dataset)
    X_small, y_small = X[:20], y[:20]
    loocv_results = CrossValidator.leave_one_out(X_small, y_small, CompositionComponent.WATER, n_components=3)
    
    print(f"\n‚úì Leave-One-Out CV (n=20):")
    print(f"  R¬≤_cv = {loocv_results['r2_cv']:.4f}")
    print(f"  RMSE_cv = {loocv_results['rmse_cv']:.4f} g/100g")
    print(f"  PRESS = {loocv_results['press']:.2f}")
    
    return True


def test_calibration_database():
    """Test calibration database with synthetic spectra."""
    print("\n" + "="*80)
    print("TEST 3: Calibration Database")
    print("="*80)
    
    # Create database
    db = CalibrationDatabase()
    db.load_from_synthetic(n_samples=50)
    
    print(f"\n‚úì Database created: {len(db)} reference spectra")
    
    # Check categories
    for category in FoodCategory:
        refs = db.get_by_category(category)
        if refs:
            print(f"  {category.value}: {len(refs)} samples")
    
    # Extract calibration data
    X_water, y_water = db.get_calibration_data(CompositionComponent.WATER)
    X_protein, y_protein = db.get_calibration_data(CompositionComponent.PROTEIN)
    X_fat, y_fat = db.get_calibration_data(CompositionComponent.FAT)
    
    print(f"\n‚úì Calibration data extracted:")
    print(f"  Water: {len(y_water)} samples, range={y_water.min():.1f}-{y_water.max():.1f} g/100g")
    print(f"  Protein: {len(y_protein)} samples, range={y_protein.min():.1f}-{y_protein.max():.1f} g/100g")
    print(f"  Fat: {len(y_fat)} samples, range={y_fat.min():.1f}-{y_fat.max():.1f} g/100g")
    
    return True


def test_composition_decoder():
    """Test full composition decoder pipeline."""
    print("\n" + "="*80)
    print("TEST 4: Full Composition Decoder")
    print("="*80)
    
    # Create decoder
    decoder = ChemicalCompositionDecoder()
    
    # Load synthetic database
    decoder.database.load_from_synthetic(n_samples=60)
    print(f"\n‚úì Loaded {len(decoder.database)} reference spectra")
    
    # Train models for major macronutrients
    components = [
        CompositionComponent.WATER,
        CompositionComponent.PROTEIN,
        CompositionComponent.FAT,
        CompositionComponent.CARBOHYDRATE
    ]
    
    decoder.train_models(components, n_components=5)
    print(f"\n‚úì Trained {len(decoder.models)} PLS models")
    
    # Test prediction on unknown sample (simulated chicken breast)
    wavelengths = np.linspace(900, 2500, 320)
    composition_true = {
        CompositionComponent.WATER: 74.0,
        CompositionComponent.PROTEIN: 23.0,
        CompositionComponent.FAT: 1.2,
        CompositionComponent.CARBOHYDRATE: 0.0
    }
    
    spectrum = decoder.database._generate_spectrum(wavelengths, composition_true)
    spectrum += np.random.normal(0, 0.005, len(wavelengths))  # Add noise
    
    # Predict
    prediction = decoder.predict_composition(wavelengths, spectrum)
    
    print(f"\n‚úì Prediction for unknown sample (simulated chicken breast):")
    print(f"\n  Component          True    Predicted  Confidence  Interval (95%)")
    print(f"  {'-'*70}")
    
    for comp in components:
        true_val = composition_true.get(comp, 0.0)
        pred_val = prediction.composition[comp]
        conf = prediction.confidence[comp]
        interval = prediction.prediction_interval[comp]
        
        print(f"  {comp.value:15s}  {true_val:6.1f}  {pred_val:6.1f}     {conf:5.1f}%     [{interval[0]:5.1f}, {interval[1]:5.1f}]")
    
    print(f"\n  Overall confidence: {prediction.get_total_confidence():.1f}%")
    
    # Calculate prediction errors
    errors = []
    for comp in components:
        true_val = composition_true.get(comp, 0.0)
        pred_val = prediction.composition[comp]
        error = abs(pred_val - true_val)
        errors.append(error)
    
    print(f"\n  Mean absolute error: {np.mean(errors):.2f} g/100g")
    print(f"  Max absolute error: {np.max(errors):.2f} g/100g")
    
    return True


def run_all_tests():
    """Run all tests."""
    print("\n" + "="*80)
    print("CHEMICAL COMPOSITION DECODER - TEST SUITE")
    print("Phase 2B: Quantitative Analysis via PLS Regression")
    print("="*80)
    
    tests = [
        ("PLS Regression", test_pls_regression),
        ("Cross-Validation", test_cross_validation),
        ("Calibration Database", test_calibration_database),
        ("Full Decoder Pipeline", test_composition_decoder),
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"\n‚úó TEST FAILED: {e}")
            results.append((test_name, False))
    
    # Summary
    print("\n" + "="*80)
    print("TEST SUMMARY")
    print("="*80)
    
    for test_name, success in results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{status}  {test_name}")
    
    passed = sum(1 for _, s in results if s)
    total = len(results)
    
    print(f"\nTotal: {passed}/{total} tests passed")
    
    if passed == total:
        print("\nüéâ All tests passed! Phase 2B complete.")
        print("\nNext: Phase 3A - Atomic Database (118 elements, 10,000+ molecules)")
    
    return passed == total


if __name__ == "__main__":
    success = run_all_tests()
    exit(0 if success else 1)
